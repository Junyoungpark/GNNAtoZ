{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a compilation of the PyTorch Geometric tutorial notebooks.\n",
    "## It compiles the following notebooks:\n",
    "## 0. graph.ipynb, 1. mp_layer.ipynb, 2. gnn.ipynb, 3. dataloader.ipyb, 4. train_gcn.ipynb\n",
    "## The original notebooks can be found at\n",
    "## https://github.com/Junyoungpark/GNNAtoZ/tree/main/pyg_tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 0 : Bring Your Own Graphs\n",
    "\n",
    "### Mathematical definition of graphs\n",
    "\n",
    "A graph is a structure amounting to a set of objects where some objects are \"related.\" The objects are often referred to as 'nodes' (or vertices), and the related pairs of vertices refer to as 'edges' (or lines). Mathematically speaking, there's no one unified framework for describing the graph. In this series of tutorials, we represent a graph as a tuple:\n",
    "\n",
    "$$\\mathcal{G}=<V,A>$$\n",
    "\n",
    "where the $V$ is the set of vertices, and $A$ is the adjacency matrix. The columns and rows indicate nodes. If the value of $A_{ij}$ is 1, then the edge exists between two nodes $i,j$. For the undirected graphs, the adjacent $A$ is symmetric.\n",
    "\n",
    "We can expand this setup to the directed graphs where the edges are directed. Depending on the edge's direction, the source node is the node where the edge starts from, and the destination node is the node where the edge reaches. In the directed graphs, the adjacent matrix $A$ can be asymmetric.\n",
    "\n",
    "Note that the index of nodes (and the edges) are arbitrary. This property becomes important in the computational process of GNN, namely permutations invariance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating a graph in python\n",
    "\n",
    "In python, we have a couple of ways to build graphs. We focus on `networkx` and `PyG` for this purpose.\n",
    "\n",
    "`networkx` is one of the basic packages for handling graph format data in python. Hence almost every major graph-related packages have an interface to convert a framework-specific graph to the `networkx` equivalent one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph() # instantiate an empty container for handling graph\n",
    "G.add_node(1) # Add node in the graph\n",
    "G.add_node(2) # Add node in the graph\n",
    "G.add_node(3) # Add node in the graph\n",
    "nx.draw_networkx(G)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's add some edges to the graph `G`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.add_edge(1,2)\n",
    "G.add_edge(2,3)\n",
    "nx.draw_networkx(G)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyG as a computational framework over graphs\n",
    "\n",
    "Even though `networkx` supports a unified way for handling graphs in python, it is not friendly with the computations. This section mathematically defines the attributed graphs where the nodes (or/and) edges have the attributes and handling such attributed graphs in `PyG`.\n",
    "\n",
    "The attributed graph $\\mathcal{G}$ is often defined as follows:\n",
    "$$\\mathcal{G} = <V, E>$$\n",
    "\n",
    "where the $V$ is the **set** of node-related features (node features), and $E$ is the **set** of edge-related features (edge features). Assume the adjacency matrix $A$ is defined implicitly.\n",
    "\n",
    "We often assume all the node features for all nodes have the same feature dimensions for the simplicity of computations. Similarly, we make the same assumption on the edge features. \n",
    "\n",
    "In practice, we achieve such assumptions easily. In some applications, different nodes may be of different types. As a result, different nodes may have different input features. However, by appending some proper dimensional vectors in the beginning or end, we can treat them to be the same dimensional vectors.\n",
    "\n",
    "Let's instanitate an attributed graph with `PyG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch_geometric\n",
    "\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.data import Data\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 4\n",
    "node_feat_dim = 9\n",
    "edge_feat_dim = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node features\n",
    "x = torch.randint(size=(num_nodes, node_feat_dim), high=3) # [num_nodes, node_feat_dim]\n",
    "\n",
    "# edges 0->1, 0->2, 0->3, 1->3, 2->1\n",
    "u, v = torch.tensor([0, 0, 0, 1, 2]), torch.tensor([1, 2, 3, 3, 1])\n",
    "edge_index = torch.stack([u, v], dim=0) # [2, num_edges]\n",
    "num_edges = edge_index.shape[1]\n",
    "\n",
    "# Edge featurs\n",
    "edge_attr = torch.randn(size=(edge_index.shape[1], \n",
    "                              edge_feat_dim)) # [num_edges, edge_feat_dim]\n",
    "\n",
    "\n",
    "g = Data(x=x, # node feature matrix\n",
    "         edge_index=edge_index,\n",
    "         edge_attr=edge_attr)\n",
    "print(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying attributes of graph\n",
    "\n",
    "We've assigned the node features as `x` and edge features as `edge_attr` to the graph `G`. We can access them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Node features \\n size: {g.x.shape} \\n values: \\n {g.x}\") # equivalently g['x']\n",
    "\n",
    "print(f\"Edge features: \\n size: {g.edge_attr.shape} \\n values: \\n {g.edge_attr}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the structure of the graph\n",
    "\n",
    "When it comes to implementing GNN, querying the statistics of graph, such as number of nodes, edges, and the degree of nodes, are often required. In this section, we will see how to query the graph structure with `PyG`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nodes in the graph\n",
    "print(f\"Number of Nodes: {g.num_nodes}\")\n",
    "\n",
    "# number of edge in the graph\n",
    "print(f\"Number of edges: {g.num_edges}\")\n",
    "\n",
    "# node feature dimension\n",
    "print(f\"Node feature dimension: {g.num_node_features}\")\n",
    "\n",
    "# edge feature dimension\n",
    "print(f\"Edge feature dimension: {g.num_edge_features}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning arbitrary attributes to the graph\n",
    "\n",
    "From time to time, we may want to store additional information to the graph. For example, the prediction label of the graph, or the graph-level features In this section, we will see how to assign arbitrary attributes to the graph with `PyG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Data(x=x, # node feature matrix\n",
    "         edge_index=edge_index,\n",
    "         edge_attr=edge_attr,\n",
    "         y_node=torch.randn(size=(num_nodes, 1)), # node label\n",
    "         y_graph=torch.randn(size=(1, 1)), # graph label       \n",
    "         )\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.y_node)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device control of `Data` object\n",
    "\n",
    "In pytorch, we can move the data to the GPU by calling `to` method. In `PyG`, we can do the same thing by calling `to` method. In this section, we will see how to move the `Data` object to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the graph to CPU\n",
    "g = g.to('cpu')\n",
    "print(g.x.device)\n",
    "\n",
    "# Move the graph to GPU\n",
    "g = g.to('cuda:0') # Move the graph to the first CUDA device -- i.e., GPU 0\n",
    "print(g.x.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed explanation of `Data` can be found from the official documentation of [link](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html#torch_geometric.data.Data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching graph with `Batch`\n",
    "\n",
    "Unlike tensor-based data, graph-based data has a variable size of the graph. \n",
    "Due to this structural differences, conventional way of batching -- i.e., compile the tensors over the new axis -- is not applicable to the graph-based data. Instead, graphs are batched with one big (disconnected) graph. In this section, we will see how to batch the graph with `Batch` object as shown in the following\n",
    "\n",
    "$$\n",
    "\\mathbf{A}=\\left[\\begin{array}{ccc}\n",
    "\\mathbf{A}_1 & & \\\\\n",
    "& \\ddots & \\\\\n",
    "& & \\mathbf{A}_n\n",
    "\\end{array}\\right], \\quad \\mathbf{X}=\\left[\\begin{array}{c}\n",
    "\\mathbf{X}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{X}_n\n",
    "\\end{array}\\right], \\quad \\mathbf{Y}=\\left[\\begin{array}{c}\n",
    "\\mathbf{Y}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{Y}_n\n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_graph(num_node: int, \n",
    "                          p_edges: float=0.1,\n",
    "                          node_feat_dim: int=3,\n",
    "                          edge_feat_dim: int=5):\n",
    "    \n",
    "    x = torch.randn(size=(num_node, node_feat_dim))\n",
    "    # assume directional and potentially self-loop\n",
    "    num_edges = int(num_node * num_node * p_edges)\n",
    "    \n",
    "    # Generate random edge index\n",
    "    # (1) Generate all possible edges\n",
    "    edge_index = torch.stack([torch.arange(num_node).repeat(num_node),\n",
    "                              torch.arange(num_node).repeat(num_node)], dim=0)\n",
    "    # (2) Shuffle the edges and take the first 'num_edges' edges\n",
    "    edge_index = edge_index[:, torch.randperm(edge_index.shape[1])]\n",
    "    edge_index = edge_index[:, :num_edges]\n",
    "    \n",
    "    edge_attr = torch.randn(size=(num_edges, edge_feat_dim))\n",
    "    \n",
    "    dummy_attr = torch.randn(size=(num_node, 1))\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr,\n",
    "                dummy_attr=dummy_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gs = 4\n",
    "num_nodes = [7, 8, 5, 11]\n",
    "\n",
    "gs = [generate_random_graph(num_node=num_node) for num_node in num_nodes]\n",
    "for i, g in enumerate(gs):\n",
    "    print(f\"Graph {i+1}: {g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch\n",
    "\n",
    "batched_g = Batch.from_data_list(gs, \n",
    "                                 exclude_keys=['dummy_attr'])\n",
    "print(type(batched_g)) # Batch is a subclass of Data -- this explains so much!)\n",
    "print(batched_g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the batched graph statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of batched graphs: {batched_g.num_graphs}\")\n",
    "print(f\"Number of cumulative nodes: {batched_g.ptr}\")\n",
    "print(f\"Batch indicators: {batched_g.batch}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 1 : Implementing GNN layer using `MessagePasing` class\n",
    "\n",
    "In general, a Graph Neural Network (GNN) layer can be written as spectral-based or spatial-based methods. The spectral-based GNN layer is defined in the Fourier domain, while the spatial-based GNN layer is defined in the vertex domain. In often cases, majority mainstream GNN models are spatial-based methods due to the limitations of spectral-based methods, including the difficulty of generalization to unseen graphs and the high computational complexity. \n",
    "\n",
    "In this tutorial, we will focus on implementing spatial-based GNN layers (i.e., message passing networks) using the `MessagePassing` class of `PyG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn.conv import MessagePassing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MessagePassing` Base Class in PyG\n",
    "\n",
    "The `MessagePassing` base class in PyG implements the message passing scheme as follows:\n",
    "\n",
    "$$\n",
    "x'_i=\n",
    "\\underbrace{\n",
    "f_\\theta \\left(\\mathbf{x}_i,\n",
    "\\underbrace{ \n",
    "\\bigoplus_{j \\in \\mathcal{N}(i)} \n",
    "}_{\\text{(2) aggregation}}\n",
    "\\underbrace{\n",
    "g_\\theta\\left(\\mathbf{x}_i, \\mathbf{x}_j, \\mathbf{e}_{ij}\\right)\n",
    "}_{\\text{(1) message}}\n",
    "\\right)\n",
    "}_{\\text{(3) update}},\n",
    "$$\n",
    "\n",
    "where \n",
    "- $x_i \\in \\mathbb{R}^{d_n}$ and $x'_i \\in \\mathbb{R}^{d_n'}$ are the input and updated node features (embeddings), respectively.\n",
    "- $d_n$ and $d'_n$ are the dimensions of node features before and after the update.\n",
    "- $e_{ij} \\in \\mathbb{R}^{d_{e}}$ is the edge feature between nodes $i$ and $j$.\n",
    "- $d_e$ is the dimension of edge features.\n",
    "- $\\bigoplus$ is a differentiable and permutation-invariant function (e.g., summation, mean, maximum).\n",
    "- $g_\\theta : \\mathbb{R}^{d_n} \\times \\mathbb{R}^{d_n} \\times \\mathbb{R}^{d_{e}} \\rightarrow \\mathbb{R}^{d''}$ is a trainable edge function (e.g., MLP).\n",
    "- $f_\\theta : \\mathbb{R}^{d_n} \\times \\mathbb{R}^{d''} \\rightarrow \\mathbb{R}^{d'_n}$ is a trainable mapping function (e.g., MLP).\n",
    "- $\\mathcal{N}(i)$ is the set of neighbors of node $i$.\n",
    "\n",
    "\n",
    "This message passing implementation is flexible and can be used to implement various GNN variants, including GCN, GAT, GraphSAGE, Interaction Layers, etc. \n",
    "\n",
    "In PyG, the `MessagePassing` triggers the (1) message, (2) message aggregation, and (3) node update functions in the order of (1) -> (2) -> (3) while `propgate` the messages. In the following tutorial, we will demonstrate how to implement the components of message passing (message, aggregation, and update) to create different graph convolution layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very First Message Passing Layer; NaiveGCN\n",
    "\n",
    "We will use the `MessagePassing` base class to implement the NaiveGCN layer that performs the following message passing scheme:\n",
    "\n",
    "$$\n",
    "x'_i = \\sigma \\left(\\sum_{j \\in \\mathcal{N}(i)} \\left(W\\mathbf{x}_j+b\\right) \\right),\n",
    "$$\n",
    "where $\\sigma$ is an non-linear activations (e.g. ReLU, Tanh, SiLU, GELU, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes this is it! We are done with the implementation of NaiveGCN.\n",
    "\n",
    "class NaiveGCNConv(MessagePassing):\n",
    "    \n",
    "    def __init__(self, dim:int, act:'str'='ReLU'):\n",
    "        \n",
    "        super().__init__(aggr='add') # Aggregates messages with Summation (i.e., addition).        \n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "        self.act = getattr(nn, act)()\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.linear(x) # Perform Wx+b\n",
    "        x = self.propagate(x=x, edge_index=edge_index) # Propagate messages and Aggregate them with Summation\n",
    "        x = self.act(x) # Apply activation function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node = 5\n",
    "n_dim = 16\n",
    "\n",
    "g = generate_random_graph(num_node=num_node, \n",
    "                          p_edges=0.5,\n",
    "                          node_feat_dim=n_dim,\n",
    "                          edge_feat_dim=n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = NaiveGCNConv(dim=n_dim)\n",
    "gc_out = conv(g.x, g.edge_index)\n",
    "print(gc_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we have a more fine-grained control over messaging passing scheme?\n",
    "\n",
    "As explained earlier, we can generally manipulate (1) message generation routine,\n",
    "(2) message aggregation routine, and (3) node update routine to implement GNN layers.\n",
    "\n",
    "In the following, we will implement such (sub) routines in `MessagePassing` class.\n",
    "Generally, (1) is done by overridding `message` method, (2) is done by specifying `aggr` in `MessagePassing` class's constructor, and (3) is done by overridding `update` method.\n",
    "\n",
    "### `message` method of `MessagePassing` class\n",
    "\n",
    "The NaiveGCN can be implemented without explicit message generation as the messages are the source node features. However, in many cases, we often employ more sophisticated message generation schemes that generates \"message\" from source, destination, and edge features (if applicable). For example, the following message generation scheme is used in \n",
    "[Edge Convolution](https://arxiv.org/abs/1801.07829):\n",
    "\n",
    "$$\n",
    "x'_i = \\max_{j \\in \\mathcal{N}(i)} h_\\theta\\left(x_i, x_j-x_i\\right),\n",
    "$$\n",
    "\n",
    "where $h_\\theta$ is an learnable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConv(MessagePassing):\n",
    "    \n",
    "    def __init__(self, dim:int):\n",
    "        super().__init__(aggr='max')\n",
    "        self.h = nn.Sequential(\n",
    "            nn.Linear(dim*2, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    \n",
    "    def message(self, x_i, x_j):\n",
    "        # By overriding this function, we can specify how messages are computed by\n",
    "        \n",
    "        # x_i is the source node and x_j is the target node\n",
    "        # x_i, x_j has shape [E, dim], where E is the number of edges\n",
    "        msg = torch.cat([x_i, x_j-x_i], dim=-1) # [E, 2*dim]\n",
    "        msg = self.h(msg) # [E, dim]\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = EdgeConv(dim=n_dim)\n",
    "gc_out = conv(g.x, g.edge_index)\n",
    "print(gc_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Update` method of `MessagePassing` class\n",
    "\n",
    "The update method is used to update the node features using the aggregated messages. As a running example, we will consider a simplified Interaction Network layer, an iconic GNN model for learning the interaction between two objects (e.g., atoms, nodes, etc.) in a graph. The update function of the Interaction Network layer is defined as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "e'_{ij} &= f_\\theta(x_i, x_j, e_{ij}), \\\\\n",
    "x'_{i} &= g_\\theta(x_i, \\sum_{j \\in \\mathcal{N}(i)} e'_{ij}),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $f_\\theta$ and $g_\\theta$ are edge and node updater, respectively. The updaters are often implemented with learnable functions (e.g., MLP)\n",
    "\n",
    "Unlike `NaiveGCNConv` or `EdgeConv`, the Interaction Network Layer requires to update the edge features to perform node updates. To take account the edge update, we additionally implement `edge_update` method in the `MessagePassing` class. This `edge_update` method is called inside of `edge_updater` method that is already defined in `MessagePassing` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionNetworkLayer(MessagePassing):\n",
    "    \n",
    "    def __init__(self, dim:int):\n",
    "        super().__init__(aggr='add')\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim), # Assuming edge are node features are of same dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        \n",
    "        self.g = nn.Sequential(\n",
    "            nn.Linear(dim*2, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        updated_ef = self.edge_updater(edge_index=edge_index, x=x, edge_attr=edge_attr)\n",
    "        updated_nf = self.propagate(edge_index=edge_index, \n",
    "                                    x=x, edge_attr=updated_ef)    \n",
    "        return updated_nf, updated_ef\n",
    "    \n",
    "    def edge_update(self, x, edge_index, edge_attr):\n",
    "        row, col = edge_index\n",
    "        x_i, x_j = x[row], x[col] # src and dst node features\n",
    "        return self.f(torch.cat([x_i, x_j, edge_attr], dim=-1)) # Eq (1)\n",
    "\n",
    "    # Eq (2) related\n",
    "    def message(self, edge_attr):\n",
    "        return edge_attr\n",
    "    \n",
    "    def update(self, aggr_msg, x): \n",
    "        # !!! The update function takes in the aggregated messages as the first argument !!!\n",
    "        # The other arguments are any arguments passed to the propagate function.\n",
    "        \n",
    "        # Maybe good to practice\n",
    "        # Try to pass additional arguments that is not passed to self.propagate such as 'y'\n",
    "        # by changing 'def update(self, aggs_msg, x)' to 'def update(self, aggs_msg, x, y)'\n",
    "        return self.g(torch.cat([x, aggr_msg], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = InteractionNetworkLayer(dim=n_dim)\n",
    "updated_nf, updated_ef = conv(g.x, g.edge_index, g.edge_attr)\n",
    "print(updated_nf.shape, updated_ef.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `aggr` argument of `messagePassing` class's constructor\n",
    "\n",
    "### Implementing `AttentiveInteractionLayer` with Advanced aggregation\n",
    "\n",
    "So far, we've considered to use \"simple\" aggregation methods in aggregating messages. However, we can consider a more sophisticated aggregation methods that can be used to implement various GNN variants. Luckily, PyG provides a set of aggregation methods that can be used to implement various GNN variants. For example, we implement attentive aggregation with `aggr.AttentionalAggregation`. Formally, the following layer performs the following message passing scheme:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "e'_{ij} &= f_\\theta(x_i, x_j, e_{ij}), \\\\\n",
    "w_{ij} &= \\text{softmax}_j \\left(\\text{gate}_\\theta(e'_{ij}) \\right), \\\\\n",
    "x'_{i} &= g_\\theta(x_i, \\sum_{j \\in \\mathcal{N}(i)} w_{ij} e'_{ij}),\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import aggr\n",
    "\n",
    "class AttentiveINLayer(MessagePassing):\n",
    "    \n",
    "    def __init__(self, dim:int):\n",
    "        \n",
    "        gate_nn = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim, 1)\n",
    "        )\n",
    "            \n",
    "        super().__init__(aggr=aggr.AttentionalAggregation(gate_nn=gate_nn))\n",
    "        \n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim), # Assuming edge are node features are of same dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        \n",
    "        self.g = nn.Sequential(\n",
    "            nn.Linear(dim*2, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        updated_ef = self.edge_updater(edge_index=edge_index, x=x, edge_attr=edge_attr)\n",
    "        updated_nf = self.propagate(edge_index, x=x, edge_attr=updated_ef)    \n",
    "        return updated_nf, updated_ef\n",
    "    \n",
    "    def edge_update(self, x, edge_index, edge_attr):\n",
    "        row, col = edge_index\n",
    "        x_i, x_j = x[row], x[col] # src and dst node features\n",
    "        return self.f(torch.cat([x_i, x_j, edge_attr], dim=-1)) # Eq (1)\n",
    "\n",
    "    def message(self, edge_attr):\n",
    "        return edge_attr\n",
    "    \n",
    "    def update(self, aggr_msg, x): \n",
    "        return self.g(torch.cat([x, aggr_msg], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = AttentiveINLayer(dim=n_dim)\n",
    "updated_nf, updated_ef = conv(g.x, g.edge_index, g.edge_attr)\n",
    "print(updated_nf.shape, updated_ef.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2 : Build Graph Neural Networks with PyG\n",
    "\n",
    "In this tutorial, we will learn how to build a graph neural network with PyG.\n",
    "PyG offers various handy features when it comes to build GNNs, including\n",
    "- An extended `Sequential` module that can be used to build GNN\n",
    "- Pre-implemented and, also, optimized graph convolutional layers\n",
    "- Graph Neural Network implementations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A limitation of `torch.nn.Sequential`\n",
    "\n",
    "In native PyTorch, `torch.nn.Sequential` is a handy module that allows us to build a neural network in a sequential manner. For example, we can build a simple MLP with `torch.nn.Sequential` as follows:\n",
    "\n",
    "```python\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    ")\n",
    "```\n",
    "\n",
    "`Sequential` class minimizes the boiler plate code for implementing `forward` methods. \n",
    "We can implement equivalent MLP without using `Sequential` as follows:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(32, 32), \n",
    "                                     nn.ReLU(), \n",
    "                                     nn.Linear(32, 1)])\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "mlp = MLP()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "However, `torch.nn.Sequential` has a limitation that each layer should have only one input and output.\n",
    "This limitation becomes a problem when it comes to building a graph neural network. For instance, when `INLayer` takes\n",
    "two inputs, node and edge features and return two outputs updated node and edge features. Hence\n",
    "it is less trivial to build a graph neural network with `torch.nn.Sequential`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `pytorch_geometric.nn.Sequential` to build GNN\n",
    "\n",
    "`pytorch_geometric.nn.Sequential` is an extended version of `torch.nn.Sequential` that allows us to build a graph neural network in a sequential manner. Let's see how we can build a graph neural network with `pytorch_geometric.nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import Sequential\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "print(help(InteractionNetworkLayer.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 5\n",
    "model = Sequential(\"x, edge_index, edge_attr\", # input\n",
    "                   [\n",
    "                       (InteractionNetworkLayer(dim), \"x, edge_index, edge_attr -> x, edge_attr\"), \n",
    "                       (InteractionNetworkLayer(dim), \"x, edge_index, edge_attr -> x, edge_attr\"),\n",
    "                   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = Batch.from_data_list([generate_random_graph(5 * (i+1),\n",
    "                                                 node_feat_dim=dim,\n",
    "                                                 edge_feat_dim=dim) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unf, uef = model(gs.x, gs.edge_index, gs.edge_attr) # update node feature (unf), updated edge feature (uef)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-implemented graph convolutional layers in PyG\n",
    "\n",
    "PyG offers various pre-implemented graph convolutional layers. Let's see how we can use them.\n",
    "In this tutorial, we will check three iconic graph convolutional layers, `GCNConv` and ` SAGEConv`.\n",
    "The exhaustive list of implemented graph convolutional layers can be found [here](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing `GCNConv` in PyG\n",
    "\n",
    "`GCNConv` is a graph convolutional layer proposed in [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907). In PyG, we can implement Graph Convolutional Network (GCN) in various way.\n",
    "We will check two different ways to implement GCN in PyG.\n",
    "- Using `GCNConv` layer with `Sequential` module\n",
    "- Using 'models.GCN' class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "out_dim = 13\n",
    "\n",
    "# Construct GCN layer (i.e., GCNConv)\n",
    "gcn_conv = GCNConv(dim, out_dim)\n",
    "updated_x = gcn_conv(gs.x, gs.edge_index)\n",
    "\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'Output node feature size: {updated_x.shape} \\n')\n",
    "\n",
    "# Construct GCN by stacking GCNConv using Sequential\n",
    "gcn = Sequential(\"x, edge_index\", \n",
    "                 [(GCNConv(dim, dim), \"x, edge_index -> x\"),\n",
    "                  (GCNConv(dim, dim), \"x, edge_index -> x\"),\n",
    "                  (GCNConv(dim, dim), \"x, edge_index -> x\"),\n",
    "                ]\n",
    ")\n",
    "\n",
    "# Or equivalently\n",
    "# num_layers = 3\n",
    "# gcn = Sequential(\"x, edge_index\", [(GCNConv(dim, dim), \"x, edge_index -> x\") for _ in range(num_layers)])\n",
    "# print(gcn)\n",
    "\n",
    "print(f'Model spec: \\n {gcn} \\n')\n",
    "\n",
    "# GCN forward\n",
    "gcn_out = gcn(gs.x, gs.edge_index)\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'GCN output node feature size: {gcn_out.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct GCN using `torch_geometric.nn.models.GCN`\n",
    "\n",
    "`PyG` provides pre-implemented famous GNN models with the enhanced features and code-level optimizations.\n",
    "`torch_geometric.nn.models.GCN` is one of the pre-implemented GCN in `PyG`. Using this we can build a GCN, by simplying\n",
    "calling `models.GCN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models import GCN\n",
    "\n",
    "gcn = GCN(in_channels=dim, \n",
    "          hidden_channels=dim, \n",
    "          out_channels=dim, num_layers=3)\n",
    "\n",
    "gcn_out = gcn(gs.x, gs.edge_index)\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'GCN output node feature size: {gcn_out.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Graph SAGE with PyG\n",
    "\n",
    "Graph SAGE is a graph convolutional layer proposed in [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "sage_conv = SAGEConv(in_channels=dim, \n",
    "                     out_channels=dim,\n",
    "                     aggr='mean')\n",
    "print(sage_conv)\n",
    "sage_out = sage_conv(gs.x, gs.edge_index)\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'Graph SAVE output node feature size: {sage_out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models import GraphSAGE\n",
    "\n",
    "sage_conv_kwargs = {\n",
    "    'aggr': 'mean'\n",
    "}\n",
    "graph_sage = GraphSAGE(in_channels=-1, # '-1' let the model infer the input dimension from the first forward!\n",
    "                       # THis can be a handy feature, BUT not recommended for readability and reproducibility\n",
    "                       hidden_channels=dim,\n",
    "                       out_channels=13,\n",
    "                       num_layers=3,\n",
    "                       **sage_conv_kwargs)\n",
    "graph_sage                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_sage_out = graph_sage(gs.x, gs.edge_index)\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'Graph SAVE output node feature size: {graph_sage_out.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Readout and Pooling\n",
    "\n",
    "So far, we've learned how to build a graph neural network with PyG. The graph neural network $f_\\theta$ generically takes a graph $\\mathcal{G}=(X,E)$ and returns the updated graph $\\mathcal{G}'=(X',E')$ as follows:\n",
    "$$\n",
    "\\mathcal{G}' = f_\\theta(\\mathcal{G})\n",
    "$$\n",
    "\n",
    "However, for some tasks, we want to map the graph to a single vector (e.g., Graph property prediction tasks, where input is a graph ans output is the scalar-represented values). In this case, we need to aggregate the node features into a single vector. This process is called graph pooling or graph readout. In this tutorial, we will learn how to implement graph pooling with PyG."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very simple pooling method; SumPooling\n",
    "\n",
    "The simplest way to aggregate node features into a single vector is to sum up all the node features. This method is called SumPooling. Mathematically, SumPooling can be defined as follows:\n",
    "$$\n",
    "x_\\mathcal{G} = \\sum_{i \\in \\mathcal{N}} x_i\n",
    "$$\n",
    "where $\\mathcal{N}$ is the set of nodes in the graph $\\mathcal{G}$. We can also consider to pool edges features (if exists) in a similar fashion. \n",
    "\n",
    "Okay, why don't we the sum pooling as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "import torch\n",
    "\n",
    "num_graphs = 3\n",
    "num_nodes = 5\n",
    "hidden_dim = 12\n",
    "\n",
    "h = torch.randn(num_graphs, num_nodes, hidden_dim)\n",
    "aggr = h.sum(dim=1) # perform summation along the first dimension\n",
    "print(aggr.shape) # torch.Size([3, 12])\n",
    "```\n",
    "\n",
    "Unfortunately, it is often impossible batching node features in a Tensor as the graphs in the batch often have different number of nodes. Therefore, we need to implement a custom SumPooling layer that can handle a batch of graphs with different number of nodes. In PyG, we can implement SumPooling as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.pool import global_add_pool, global_mean_pool, global_max_pool # Yes, you can do mean, max pooling in PyG!\n",
    "\n",
    "pooled = global_add_pool(x=gs.x, \n",
    "                         batch=gs.batch) # batch is a tensor that indicates which graph the node belongs to\n",
    "pooled.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: `pytorch_scatter` for more complex pooling routines\n",
    "\n",
    "`pytorch_geometric` used to implement several key features with `pytorch_scatter` for 'pooling' (i.e., aggregate the set of vectors into a single vectors). In this section, we will learn how to use `pytorch_scatter` for more complex pooling routines.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./assets/add.svg\" alt=\"Image description\" style=\"width: 400px;\">\n",
    "  <p style=\"margin-top: 10px;\">The behavior of torch_scatter.scatter </p>\n",
    "</div>\n",
    "\n",
    "Figure from [here](https://pytorch-scatter.readthedocs.io/en/latest/functions/add.html)\n",
    "\n",
    "As you can see from the figure, `scatter` operation aggregates 'src' (or input) into 'outputs' with the 'index' vectors. This design choice\n",
    "allows store the data with plain tensor while aggregating the different number of inputs with a single operation.\n",
    "\n",
    "**Note: the same features are now implemented in `torch_geometric.utils.scatter`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_scatter import scatter\n",
    "# or equivalently\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "dim1, dim2 = 11, 13\n",
    "src = torch.randn(6, dim1, dim2)\n",
    "index = torch.tensor([0, 1, 0, 1, 2, 1])\n",
    "\n",
    "# Naive loopy implementation\n",
    "out_naive = torch.zeros(index.unique().shape[0], dim1, dim2)\n",
    "for i in index.unique().tolist():\n",
    "    out_naive[i] = src[index == i].sum(dim=0)\n",
    "    \n",
    "# torch_scatter implementation\n",
    "out = scatter(src, index, dim=0, reduce=\"sum\") # reduce can be \"sum\", \"mean\", \"max\", \"min\"\n",
    "\n",
    "assert torch.allclose(out_naive, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import segment\n",
    "# A similar operation can be done also based on the segments\n",
    "# For the further details, please refer to the documentation\n",
    "\n",
    "src = torch.randn(10, 6, 64)\n",
    "indptr = torch.tensor([0, 2, 5, 6])\n",
    "indptr = indptr.view(1, -1)  # Broadcasting in the first and last dim.\n",
    "\n",
    "out = segment(src, indptr, reduce=\"sum\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite scatter operations; `Softmax`, `logsumexp`, and `scatter_std`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter.composite import scatter_softmax, scatter_std, scatter_logsumexp\n",
    "# or equivalently\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "src = torch.randn(10, 1)\n",
    "idx = torch.tensor([0,0,0,1,1,2,2,2,2,2])\n",
    "out = scatter_softmax(src, idx, dim=0)\n",
    "print(out.shape)\n",
    "\n",
    "print(\"Results of 'scatter_softmax'\")\n",
    "print(f'1st batch: {out[:3].view(-1)}, sum={out[:3].sum()}')\n",
    "print(f'2nd batch: {out[3:5].view(-1)}, sum={out[3:5].sum()}')\n",
    "print(f'3rd batch: {out[5:].view(-1)}, sum={out[5:].sum()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative to the graph pooling, **Virtual node** is also often used to concentrate node/edge feature to a single vector. \n",
    "A primitive approach to virtual node is introducing additional node to the graph and attach the edges from the virtual node to all the nodes in the graph. \n",
    "It often works well, as compared to the naive graph pooling approaches. Please refer to the research papers for more details:\n",
    "- [Graph Classification via Deep Learning with Virtual Nodes](https://arxiv.org/pdf/1708.04357.pdf),\n",
    "- [On the Connection Between MPNN and Graph Transformer](https://arxiv.org/pdf/2301.11956.pdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 3 : Interfacing Graph `Data` with Pytorch Geometric `DataLoader`\n",
    "\n",
    "In this tutorial, we will learn how to use Pytorch Geometric `DataLoader` to load graph data for mini-batch training. \\\n",
    "Q) Why we want to use Pytorch Geometric `DataLoader`? \\\n",
    "A) It is inherited from Pytorch `DataLoader`, which means it is easy to use + supported by Pytorch community.\n",
    "\n",
    "Let's start the tutorial by implementing graph dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch_geometric.utils import erdos_renyi_graph\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_er_graph(num_nodes:int, \n",
    "                      edge_prob:float,\n",
    "                      feat_dim:int=16):\n",
    "    \n",
    "    edge_idx = erdos_renyi_graph(num_nodes=num_nodes,edge_prob=edge_prob)\n",
    "    x = torch.randn(num_nodes, feat_dim)\n",
    "    y = (x.sum() / num_nodes).view(1,1)\n",
    "    dummy = torch.randn(num_nodes, 32)\n",
    "    g = Data(x=x, edge_index=edge_idx, y=y, dummy=dummy)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 num_graphs:int,\n",
    "                 min_num_nodes: int = 32,\n",
    "                 max_num_nodes: int = 64,\n",
    "                 edge_prob: float = 0.3):\n",
    "        \n",
    "        num_nodes = torch.randint(min_num_nodes, max_num_nodes, (num_graphs,))\n",
    "        self.gs = [\n",
    "            generate_er_graph(num_nodes[i], edge_prob) for i in range(num_graphs)\n",
    "        ]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.gs[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ERDataset(128)\n",
    "print(dataset[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyG `DataLoader`\n",
    "\n",
    "As mentioned earlier, PyG `DataLoader` is inherited from Pytorch `DataLoader`. Meaning that we can\n",
    "pass any arguments or keyword agruments that Pytorch `DataLoader` supports. Furthermore, PyG `DataLoader`\n",
    "supports graph (mini) batching using custom `collate_fn` -- basically, an well engineering version of `Batch.from_data_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, \n",
    "                        follow_batch=['batch'], # we can specify which attributes to follow to form \"batch\" attribute\n",
    "                        exclude_keys=['dummy'], # we can specify which attributes can be excluded from the batch\n",
    "                        batch_size=32, shuffle=True)\n",
    "batched_g = next(iter(dataloader))\n",
    "\n",
    "print(batched_g)\n",
    "print(f'Number of graphs in batch: {batched_g.num_graphs}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Batched Data to a GNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models import GCN\n",
    "\n",
    "model = GCN(in_channels=16,\n",
    "            hidden_channels=32, \n",
    "            out_channels=1,\n",
    "            num_layers=3)\n",
    "\n",
    "pred = model(batched_g.x, batched_g.edge_index)\n",
    "print(pred.shape) # [#. total nodes, output dim]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 4: Minimalistic re-implementation of Graph Convolutional Networks (GCN) in PyG\n",
    "\n",
    "In this tutorial, we will reimplement Semi-Supervised Classification with Graph Convolutional Networks (GCN) introduced by [Kipf et al. (2017)](https://arxiv.org/abs/1609.02907) with PyTorch Geometric. The following codes are inspired by an open source implementation [here](https://github.com/ki-ljl/PyG-GCN/tree/main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import Planetoid, NELL\n",
    "from torch_geometric.nn.models import GCN\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(name: str):\n",
    "    assert name in ['Cora', 'CiteSeer', 'PubMed']\n",
    "    dataset = Planetoid(root=f'/tmp/{name}', name=f'{name}')\n",
    "    return dataset \n",
    "\n",
    "def train(model, data, \n",
    "          num_epochs:int=200,\n",
    "          device:str='cpu'):\n",
    "    model = model.to(device)\n",
    "    data = data.to(device)\n",
    "    \n",
    "    opt = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    model.train() # Set model to 'train' mode\n",
    "    \n",
    "    pbar = tqdm(range(num_epochs), total=num_epochs, ascii=' =', leave=True)\n",
    "    for epoch in pbar:\n",
    "        out = model(data.x, data.edge_index)\n",
    "        opt.zero_grad()\n",
    "        loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Progress bar\n",
    "        pbar.set_description('Epoch {:03d} loss {:.4f}'.format(epoch, loss.item()))\n",
    "        \n",
    "    model = model.to('cpu')\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    _, pred = model(data.x, data.edge_index).max(dim=1)\n",
    "    correct = int(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())\n",
    "    acc = correct / int(data.test_mask.sum())\n",
    "    print('Accuracy: {:.4f}'.format(acc))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_names = ['Cora', 'CiteSeer', 'PubMed']\n",
    "device = 'cuda:0'\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    dataset = get_dataset(dataset_name)\n",
    "\n",
    "    model = GCN(in_channels=dataset.num_node_features, \n",
    "                hidden_channels=32, \n",
    "                num_layers=2,\n",
    "                out_channels=dataset.num_classes,\n",
    "                dropout=0.5,\n",
    "                norm='batch', # 'batch', 'instance', 'layer', 'none'\n",
    "                )\n",
    "\n",
    "    train(model, dataset[0], device=device)\n",
    "    print(f'--- {dataset_name} ---')\n",
    "    test(model, dataset[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch200-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
