{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2 : Build Graph Neural Networks with PyG\n",
    "\n",
    "In this tutorial, we will learn how to build a graph neural network with PyG.\n",
    "PyG offers various handy features when it comes to build GNNs, including\n",
    "- An extended `Sequential` module that can be used to build GNN\n",
    "- Pre-implemented and, also, optimized graph convolutional layers\n",
    "- Graph Neural Network implementations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A limitation of `torch.nn.Sequential`\n",
    "\n",
    "In native PyTorch, `torch.nn.Sequential` is a handy module that allows us to build a neural network in a sequential manner. For example, we can build a simple MLP with `torch.nn.Sequential` as follows:\n",
    "\n",
    "```python\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    ")\n",
    "```\n",
    "\n",
    "`Sequential` class minimizes the boiler plate code for implementing `forward` methods. \n",
    "We can implement equivalent MLP without using `Sequential` as follows:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(32, 32), \n",
    "                                     nn.ReLU(), \n",
    "                                     nn.Linear(32, 1)])\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "mlp = MLP()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "However, `torch.nn.Sequential` has a limitation that each layer should have only one input and output.\n",
    "This limitation becomes a problem when it comes to building a graph neural network. For instance, when `INLayer` takes\n",
    "two inputs, node and edge features and return two outputs updated node and edge features. Hence\n",
    "it is less trivial to build a graph neural network with `torch.nn.Sequential`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `pytorch_geometric.nn.Sequential` to build GNN\n",
    "\n",
    "`pytorch_geometric.nn.Sequential` is an extended version of `torch.nn.Sequential` that allows us to build a graph neural network in a sequential manner. Let's see how we can build a graph neural network with `pytorch_geometric.nn.Sequential`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function forward in module common.layers:\n",
      "\n",
      "forward(self, x, edge_index, edge_attr) -> Tuple[torch.Tensor, torch.Tensor]\n",
      "    Runs the forward pass of the module.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import Sequential\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "from common.graph_gen import generate_random_graph\n",
    "from common.layers import InteractionNetworkLayer\n",
    "\n",
    "print(help(InteractionNetworkLayer.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 5\n",
    "model = Sequential(\"x, edge_index, edge_attr\", # input\n",
    "                   [\n",
    "                       (InteractionNetworkLayer(dim), \"x, edge_index, edge_attr -> x, edge_attr\"), \n",
    "                       (InteractionNetworkLayer(dim), \"x, edge_index, edge_attr -> x, edge_attr\"),\n",
    "                   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = Batch.from_data_list([generate_random_graph(5 * (i+1),\n",
    "                                                 node_feat_dim=dim,\n",
    "                                                 edge_feat_dim=dim) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unf, uef = model(gs.x, gs.edge_index, gs.edge_attr) # update node feature (unf), updated edge feature (uef)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-implemented graph convolutional layers in PyG\n",
    "\n",
    "PyG offers various pre-implemented graph convolutional layers. Let's see how we can use them.\n",
    "In this tutorial, we will check three iconic graph convolutional layers, `GCNConv` and ` SAGEConv`.\n",
    "The exhaustive list of implemented graph convolutional layers can be found [here](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing `GCNConv` in PyG\n",
    "\n",
    "`GCNConv` is a graph convolutional layer proposed in [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907). In PyG, we can implement Graph Convolutional Network (GCN) in various way.\n",
    "We will check two different ways to implement GCN in PyG.\n",
    "- Using `GCNConv` layer with `Sequential` module\n",
    "- Using 'models.GCN' class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input node feature size: torch.Size([30, 5])\n",
      "Output node feature size: torch.Size([30, 13]) \n",
      "\n",
      "Model spec: \n",
      " Sequential(\n",
      "  (0): GCNConv(5, 5)\n",
      "  (1): GCNConv(5, 5)\n",
      "  (2): GCNConv(5, 5)\n",
      ") \n",
      "\n",
      "Input node feature size: torch.Size([30, 5])\n",
      "GCN output node feature size: torch.Size([30, 5])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "out_dim = 13\n",
    "\n",
    "# Construct GCN layer (i.e., GCNConv)\n",
    "gcn_conv = GCNConv(dim, out_dim)\n",
    "updated_x = gcn_conv(gs.x, gs.edge_index)\n",
    "\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'Output node feature size: {updated_x.shape} \\n')\n",
    "\n",
    "# Construct GCN by stacking GCNConv using Sequential\n",
    "gcn = Sequential(\"x, edge_index\", \n",
    "                 [(GCNConv(dim, dim), \"x, edge_index -> x\"),\n",
    "                  (GCNConv(dim, dim), \"x, edge_index -> x\"),\n",
    "                  (GCNConv(dim, dim), \"x, edge_index -> x\"),\n",
    "                ]\n",
    ")\n",
    "\n",
    "# Or equivalently\n",
    "# num_layers = 3\n",
    "# gcn = Sequential(\"x, edge_index\", [(GCNConv(dim, dim), \"x, edge_index -> x\") for _ in range(num_layers)])\n",
    "# print(gcn)\n",
    "\n",
    "print(f'Model spec: \\n {gcn} \\n')\n",
    "\n",
    "# GCN forward\n",
    "gcn_out = gcn(gs.x, gs.edge_index)\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'GCN output node feature size: {gcn_out.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct GCN using `torch_geometric.nn.models.GCN`\n",
    "\n",
    "`PyG` provides pre-implemented famous GNN models with the enhanced features and code-level optimizations.\n",
    "`torch_geometric.nn.models.GCN` is one of the pre-implemented GCN in `PyG`. Using this we can build a GCN, by simplying\n",
    "calling `models.GCN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input node feature size: torch.Size([30, 5])\n",
      "GCN output node feature size: torch.Size([30, 5])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn.models import GCN\n",
    "\n",
    "gcn = GCN(in_channels=dim, \n",
    "          hidden_channels=dim, \n",
    "          out_channels=dim, num_layers=3)\n",
    "\n",
    "gcn_out = gcn(gs.x, gs.edge_index)\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'GCN output node feature size: {gcn_out.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Graph SAGE with PyG\n",
    "\n",
    "Graph SAGE is a graph convolutional layer proposed in [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAGEConv(5, 5, aggr=mean)\n",
      "Input node feature size: torch.Size([30, 5])\n",
      "Graph SAVE output node feature size: torch.Size([30, 5])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "sage_conv = SAGEConv(in_channels=dim, \n",
    "                     out_channels=dim,\n",
    "                     aggr='mean')\n",
    "print(sage_conv)\n",
    "sage_out = sage_conv(gs.x, gs.edge_index)\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'Graph SAVE output node feature size: {sage_out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphSAGE(-1, 13, num_layers=3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn.models import GraphSAGE\n",
    "\n",
    "sage_conv_kwargs = {\n",
    "    'aggr': 'mean'\n",
    "}\n",
    "graph_sage = GraphSAGE(in_channels=-1, # '-1' let the model infer the input dimension from the first forward!\n",
    "                       # THis can be a handy feature but not recommended for readability and reproducibility\n",
    "                       hidden_channels=dim,\n",
    "                       out_channels=13,\n",
    "                       num_layers=3,\n",
    "                       **sage_conv_kwargs)\n",
    "graph_sage                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input node feature size: torch.Size([30, 5])\n",
      "Graph SAVE output node feature size: torch.Size([30, 13])\n"
     ]
    }
   ],
   "source": [
    "graph_sage_out = graph_sage(gs.x, gs.edge_index)\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'Graph SAVE output node feature size: {graph_sage_out.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Readout and Pooling\n",
    "\n",
    "So far, we've learned how to build a graph neural network with PyG. The graph neural network $f_\\theta$ generically takes a graph $\\mathcal{G}=(X,E)$ and returns the updated graph $\\mathcal{G}'=(X',E')$ as follows:\n",
    "$$\n",
    "\\mathcal{G}' = f_\\theta(\\mathcal{G})\n",
    "$$\n",
    "\n",
    "However, for some tasks, we want to map the graph to a single vector (e.g., Graph property prediction tasks, where input is a graph ans output is the scalar-represented values). In this case, we need to aggregate the node features into a single vector. This process is called graph pooling or graph readout. In this tutorial, we will learn how to implement graph pooling with PyG."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very simple pooling method; SumPooling\n",
    "\n",
    "The simplest way to aggregate node features into a single vector is to sum up all the node features. This method is called SumPooling. Mathematically, SumPooling can be defined as follows:\n",
    "$$\n",
    "x_\\mathcal{G} = \\sum_{i \\in \\mathcal{N}} x_i\n",
    "$$\n",
    "where $\\mathcal{N}$ is the set of nodes in the graph $\\mathcal{G}$. We can also consider to pool edges features (if exists) in a similar fashion. \n",
    "\n",
    "Okay, why don't we the sum pooling as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "import torch\n",
    "\n",
    "num_graphs = 3\n",
    "num_nodes = 5\n",
    "hidden_dim = 12\n",
    "\n",
    "h = torch.randn(num_graphs, num_nodes, hidden_dim)\n",
    "aggr = h.sum(dim=1) # perform summation along the first dimension\n",
    "print(aggr.shape) # torch.Size([3, 12])\n",
    "```\n",
    "\n",
    "Unfortunately, it is often impossible batching node features in a Tensor as the graphs in the batch often have different number of nodes. Therefore, we need to implement a custom SumPooling layer that can handle a batch of graphs with different number of nodes. In PyG, we can implement SumPooling as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn.pool import global_add_pool, global_mean_pool, global_max_pool # Yes, you can do mean, max pooling in PyG!\n",
    "\n",
    "pooled = global_add_pool(x=gs.x, \n",
    "                         batch=gs.batch) # batch is a tensor that indicates which graph the node belongs to\n",
    "pooled.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: `pytorch_scatter` for more complex pooling routines\n",
    "\n",
    "`pytorch_geometric` used to implement several key features with `pytorch_scatter` for 'pooling' (i.e., aggregate the set of vectors into a single vectors). In this section, we will learn how to use `pytorch_scatter` for more complex pooling routines.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./assets/add.svg\" alt=\"Image description\" style=\"width: 400px;\">\n",
    "  <p style=\"margin-top: 10px;\">The behavior of torch_scatter.scatter </p>\n",
    "</div>\n",
    "\n",
    "Figure from [here](https://pytorch-scatter.readthedocs.io/en/latest/functions/add.html)\n",
    "\n",
    "As you can see from the figure, `scatter` operation aggregates 'src' (or input) into 'outputs' with the 'index' vectors. This design choice\n",
    "allows store the data with plain tensor while aggregating the different number of inputs with a single operation.\n",
    "\n",
    "**Note: the same features are now implemented in `torch_geometric.utils.scatter`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_scatter import scatter\n",
    "# or equivalently\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "dim1, dim2 = 11, 13\n",
    "src = torch.randn(6, dim1, dim2)\n",
    "index = torch.tensor([0, 1, 0, 1, 2, 1])\n",
    "\n",
    "# Naive loopy implementation\n",
    "out_naive = torch.zeros(index.unique().shape[0], dim1, dim2)\n",
    "for i in index.unique().tolist():\n",
    "    out_naive[i] = src[index == i].sum(dim=0)\n",
    "    \n",
    "# torch_scatter implementation\n",
    "out = scatter(src, index, dim=0, reduce=\"sum\") # reduce can be \"sum\", \"mean\", \"max\", \"min\"\n",
    "\n",
    "assert torch.allclose(out_naive, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import segment\n",
    "# A similar operation can be done also based on the segments\n",
    "# For the further details, please refer to the documentation\n",
    "\n",
    "src = torch.randn(10, 6, 64)\n",
    "indptr = torch.tensor([0, 2, 5, 6])\n",
    "indptr = indptr.view(1, -1)  # Broadcasting in the first and last dim.\n",
    "\n",
    "out = segment(src, indptr, reduce=\"sum\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite scatter operations; `Softmax`, `logsumexp`, and `scatter_std`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "Results of 'scatter_softmax'\n",
      "1st batch: tensor([0.2878, 0.1079, 0.6043]), sum=0.9999999403953552\n",
      "2nd batch: tensor([0.8721, 0.1279]), sum=0.9999999403953552\n",
      "3rd batch: tensor([0.0951, 0.1077, 0.2047, 0.4446, 0.1479]), sum=1.0\n"
     ]
    }
   ],
   "source": [
    "from torch_scatter.composite import scatter_softmax, scatter_std, scatter_logsumexp\n",
    "# or equivalently\n",
    "from torch_geometric.utils import softmax\n",
    "\n",
    "src = torch.randn(10, 1)\n",
    "idx = torch.tensor([0,0,0,1,1,2,2,2,2,2])\n",
    "out = scatter_softmax(src, idx, dim=0)\n",
    "print(out.shape)\n",
    "\n",
    "print(\"Results of 'scatter_softmax'\")\n",
    "print(f'1st batch: {out[:3].view(-1)}, sum={out[:3].sum()}')\n",
    "print(f'2nd batch: {out[3:5].view(-1)}, sum={out[3:5].sum()}')\n",
    "print(f'3rd batch: {out[5:].view(-1)}, sum={out[5:].sum()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternative to the graph pooling, **Virtual node** is also often used to concentrate node/edge feature to a single vector. \n",
    "A primitive approach to virtual node is introducing additional node to the graph and attach the edges from the virtual node to all the nodes in the graph. \n",
    "It often works well, as compared to the naive graph pooling approaches. Please refer to the research papers for more details:\n",
    "- [Graph Classification via Deep Learning with Virtual Nodes](https://arxiv.org/pdf/1708.04357.pdf),\n",
    "- [On the Connection Between MPNN and Graph Transformer](https://arxiv.org/pdf/2301.11956.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch200-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
