{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2 : Build Graph Neural Networks with PyG\n",
    "\n",
    "In this tutorial, we will learn how to build a graph neural network with PyG.\n",
    "PyG offers various handy features when it comes to build GNNs, including\n",
    "- An extended `Sequential` module that can be used to build GNN\n",
    "- Pre-implemented and, also, optimized graph convolutional layers\n",
    "- Graph Neural Network implementations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A limitation of `torch.nn.Sequential`\n",
    "\n",
    "In native PyTorch, `torch.nn.Sequential` is a handy module that allows us to build a neural network in a sequential manner. For example, we can build a simple MLP with `torch.nn.Sequential` as follows:\n",
    "\n",
    "```python\n",
    "mlp = nn.Sequential(\n",
    "    nn.Linear(32, 32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32, 1),\n",
    ")\n",
    "```\n",
    "\n",
    "`Sequential` class minimizes the boiler plate code for implementing `forward` methods. \n",
    "We can implement equivalent MLP without using `Sequential` as follows:\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(32, 32), \n",
    "                                     nn.ReLU(), \n",
    "                                     nn.Linear(32, 1)])\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "mlp = MLP()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "However, `torch.nn.Sequential` has a limitation that each layer should have only one input and output.\n",
    "This limitation becomes a problem when it comes to building a graph neural network. For instance, when `INLayer` takes\n",
    "two inputs, node and edge features and return two outputs updated node and edge features. Hence\n",
    "it is less trivial to build a graph neural network with `torch.nn.Sequential`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `pytorch_geometric.nn.Sequential` to build GNN\n",
    "\n",
    "`pytorch_geometric.nn.Sequential` is an extended version of `torch.nn.Sequential` that allows us to build a graph neural network in a sequential manner. Let's see how we can build a graph neural network with `pytorch_geometric.nn.Sequential`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import Sequential\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "from common.graph_gen import generate_random_graph\n",
    "from common.layers import InteractionNetworkLayer\n",
    "\n",
    "print(help(InteractionNetworkLayer.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 5\n",
    "model = Sequential(\"x, edge_index, edge_attr\", # input\n",
    "                   [\n",
    "                       (InteractionNetworkLayer(dim), \"x, edge_index, edge_attr -> x, edge_attr\"), \n",
    "                       (InteractionNetworkLayer(dim), \"x, edge_index, edge_attr -> x, edge_attr\"),\n",
    "                   ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = Batch.from_data_list([generate_random_graph(5 * (i+1),\n",
    "                                                 node_feat_dim=dim,\n",
    "                                                 edge_feat_dim=dim) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unf, uef = model(gs.x, gs.edge_index, gs.edge_attr) # update node feature (unf), updated edge feature (uef)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-implemented graph convolutional layers in PyG\n",
    "\n",
    "PyG offers various pre-implemented graph convolutional layers. Let's see how we can use them.\n",
    "In this tutorial, we will check three iconic graph convolutional layers, `GCNConv` and ` SAGEConv`.\n",
    "The exhaustive list of implemented graph convolutional layers can be found [here](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#convolutional-layers)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing `GCNConv` in PyG\n",
    "\n",
    "`GCNConv` is a graph convolutional layer proposed in [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907). In PyG, we can implement Graph Convolutional Network (GCN) in various way.\n",
    "We will check two different ways to implement GCN in PyG.\n",
    "- Using `GCNConv` layer with `Sequential` module\n",
    "- Using 'models.GCN' class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "out_dim = 13\n",
    "\n",
    "# Construct GCN layer (i.e., GCNConv)\n",
    "gcn_conv = GCNConv(dim, out_dim)\n",
    "updated_x = gcn_conv(gs.x, gs.edge_index)\n",
    "\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'Output node feature size: {updated_x.shape} \\n')\n",
    "\n",
    "# Construct GCN by stacking GCNConv using Sequential\n",
    "gcn = Sequential(\"x, edge_index\", \n",
    "                 [(GCNConv(dim, dim), \"x, edge_index -> x\"),\n",
    "                  (GCNConv(dim, dim), \"x, edge_index -> x\"),\n",
    "                  (GCNConv(dim, dim), \"x, edge_index -> x\"),\n",
    "                ]\n",
    ")\n",
    "\n",
    "# Or equivalently\n",
    "# num_layers = 3\n",
    "# gcn = Sequential(\"x, edge_index\", [(GCNConv(dim, dim), \"x, edge_index -> x\") for _ in range(num_layers)])\n",
    "# print(gcn)\n",
    "\n",
    "print(f'Model spec: \\n {gcn} \\n')\n",
    "\n",
    "# GCN forward\n",
    "gcn_out = gcn(gs.x, gs.edge_index)\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'GCN output node feature size: {gcn_out.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct GCN using `torch_geometric.nn.models.GCN`\n",
    "\n",
    "`PyG` provides pre-implemented famous GNN models with the enhanced features and code-level optimizations.\n",
    "`torch_geometric.nn.models.GCN` is one of the pre-implemented GCN in `PyG`. Using this we can build a GCN, by simplying\n",
    "calling `models.GCN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models import GCN\n",
    "\n",
    "gcn = GCN(in_channels=dim, \n",
    "          hidden_channels=dim, \n",
    "          out_channels=dim, num_layers=3)\n",
    "\n",
    "gcn_out = gcn(gs.x, gs.edge_index)\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'GCN output node feature size: {gcn_out.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Graph SAGE with PyG\n",
    "\n",
    "Graph SAGE is a graph convolutional layer proposed in [Inductive Representation Learning on Large Graphs](https://arxiv.org/abs/1706.02216). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "sage_conv = SAGEConv(in_channels=dim, \n",
    "                     out_channels=dim,\n",
    "                     aggr='mean')\n",
    "print(sage_conv)\n",
    "sage_out = sage_conv(gs.x, gs.edge_index)\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'Graph SAVE output node feature size: {sage_out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.models import GraphSAGE\n",
    "\n",
    "sage_conv_kwargs = {\n",
    "    'aggr': 'mean'\n",
    "}\n",
    "graph_sage = GraphSAGE(in_channels=-1, # '-1' let the model infer the input dimension from the first forward!\n",
    "                       # THis can be a handy feature but not recommended for readability and reproducibility\n",
    "                       hidden_channels=dim,\n",
    "                       out_channels=13,\n",
    "                       num_layers=3,\n",
    "                       **sage_conv_kwargs)\n",
    "graph_sage                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_sage_out = graph_sage(gs.x, gs.edge_index)\n",
    "print(f'Input node feature size: {gs.x.shape}')\n",
    "print(f'Graph SAVE output node feature size: {graph_sage_out.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Readout and Pooling\n",
    "\n",
    "So far, we've learned how to build a graph neural network with PyG. The graph neural network $f_\\theta$ generically takes a graph $\\mathcal{G}=(X,E)$ and returns the updated graph $\\mathcal{G}'=(X',E')$ as follows:\n",
    "$$\n",
    "\\mathcal{G}' = f_\\theta(\\mathcal{G})\n",
    "$$\n",
    "\n",
    "However, for some tasks, we want to map the graph to a single vector (e.g., Graph property prediction tasks, where input is a graph ans output is the scalar-represented values). In this case, we need to aggregate the node features into a single vector. This process is called graph pooling or graph readout. In this tutorial, we will learn how to implement graph pooling with PyG."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very simple pooling method; SumPooling\n",
    "\n",
    "The simplest way to aggregate node features into a single vector is to sum up all the node features. This method is called SumPooling. Mathematically, SumPooling can be defined as follows:\n",
    "$$\n",
    "x_\\mathcal{G} = \\sum_{i \\in \\mathcal{N}} x_i\n",
    "$$\n",
    "where $\\mathcal{N}$ is the set of nodes in the graph $\\mathcal{G}$. We can also consider to pool edges features (if exists) in a similar fashion. \n",
    "\n",
    "Okay, why don't we the sum pooling as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "import torch\n",
    "\n",
    "num_graphs = 3\n",
    "num_nodes = 5\n",
    "hidden_dim = 12\n",
    "\n",
    "h = torch.randn(num_graphs, num_nodes, hidden_dim)\n",
    "aggr = h.sum(dim=1) # perform summation along the first dimension\n",
    "print(aggr.shape) # torch.Size([3, 12])\n",
    "```\n",
    "\n",
    "Unfortunately, it is often impossible batching node features in a Tensor as the graphs in the batch often have different number of nodes. Therefore, we need to implement a custom SumPooling layer that can handle a batch of graphs with different number of nodes. In PyG, we can implement SumPooling as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn.pool import global_add_pool, global_mean_pool, global_max_pool # Yes, you can do mean, max pooling in PyG!\n",
    "\n",
    "pooled = global_add_pool(x=gs.x, \n",
    "                         batch=gs.batch) # batch is a tensor that indicates which graph the node belongs to\n",
    "pooled.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced: `pytorch_scatter` for more complex pooling routines\n",
    "\n",
    "`pytorch_geometric` used to implement several key features with `pytorch_scatter` for 'pooling' (i.e., aggregate the set of vectors into a single vectors). In this section, we will learn how to use `pytorch_scatter` for more complex pooling routines.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "  <img src=\"./assets/add.svg\" alt=\"Image description\" style=\"width: 400px;\">\n",
    "  <p style=\"margin-top: 10px;\">The behavior of torch_scatter.scatter </p>\n",
    "</div>\n",
    "\n",
    "Figure from [here](https://pytorch-scatter.readthedocs.io/en/latest/functions/add.html)\n",
    "\n",
    "As you can see from the figure, `scatter` operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_mean, scatter_max, scatter_add"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch200-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
