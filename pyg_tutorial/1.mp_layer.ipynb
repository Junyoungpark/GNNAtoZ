{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 1 : Implementing GNN layer using `MessagePasing` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from common.graph_gen import generate_random_graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MessagePassing` Base Class in PyG\n",
    "\n",
    "The `MessagePassing` base class in PyG implements the message passing scheme as follows:\n",
    "\n",
    "$$\n",
    "x'_i=\n",
    "\\underbrace{\n",
    "f_\\theta \\left(\\mathbf{x}_i,\n",
    "\\underbrace{ \n",
    "\\bigoplus_{j \\in \\mathcal{N}(i)} \n",
    "}_{\\text{(2) aggregation}}\n",
    "\\underbrace{\n",
    "g_\\theta\\left(\\mathbf{x}_i, \\mathbf{x}_j, \\mathbf{e}_{ij}\\right)\n",
    "}_{\\text{(1) message}}\n",
    "\\right)\n",
    "}_{\\text{(3) update}},\n",
    "$$\n",
    "\n",
    "where \n",
    "- $x_i \\in \\mathbb{R}^{d_n}$ and $x'_i \\in \\mathbb{R}^{d_n'}$ are the input and updated node features (embeddings), respectively.\n",
    "- $e_{ij} \\in \\mathbb{R}^{d_{e}}$ is the edge feature between nodes $i$ and $j$.\n",
    "- $\\bigoplus$ is a differentiable and permutation-invariant function (e.g., summation, mean, maximum).\n",
    "- $g_\\theta : \\mathbb{R}^{d_n} \\times \\mathbb{R}^{d_n} \\times \\mathbb{R}^{d_{e}} \\rightarrow \\mathbb{R}^{d''}$ is a trainable edge function (e.g., MLP).\n",
    "- $f_\\theta : \\mathbb{R}^{d_n} \\times \\mathbb{R}^{d''} \\rightarrow \\mathbb{R}^{d'_n}$ is a trainable mapping function (e.g., MLP).\n",
    "- $\\mathcal{N}(i)$ is the set of neighbors of node $i$.\n",
    "\n",
    "\n",
    "This message passing implementation is flexible and can be used to implement various GNN variants, including GCN, GAT, GraphSAGE, Interaction Layers, etc. In PyG, the `MessagePassing` triggers the message, aggregation, and update functions in the order of (1) -> (2) -> (3) while `propgate` the messages. In the following tutorial, we will demonstrate how to implement the components of message passing (message, aggregation, and update) to create different graph convolution layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very First Message Passing Layer; NaiveGCN\n",
    "\n",
    "We will use the `MessagePassing` base class to implement the NaiveGCN layer that performs the following message passing scheme:\n",
    "\n",
    "$$\n",
    "x'_i = \\sigma \\left(\\sum_{j \\in \\mathcal{N}(i)} \\left(W\\mathbf{x}_j+b\\right) \\right),\n",
    "$$\n",
    "where $\\sigma$ is an non-linear activations (e.g. ReLU, Tanh, SiLU, GELU, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes this is it! We are done with the implementation of NaiveGCN.\n",
    "\n",
    "class NaiveGCN(MessagePassing):\n",
    "    \n",
    "    def __init__(self, dim:int, act:'str'='ReLU'):\n",
    "        \n",
    "        super().__init__(aggr='add') # Aggregates messages with Summation (i.e., addition).        \n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "        self.act = getattr(nn, act)()\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.linear(x) # Perform Wx+b\n",
    "        x = self.propagate(x=x, edge_index=edge_index) # Propagate messages and Aggregate them with Summation\n",
    "        x = self.act(x) # Apply activation function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node = 11\n",
    "n_dim = 16\n",
    "\n",
    "g = generate_random_graph(num_node=num_node, \n",
    "                          node_feat_dim=n_dim,\n",
    "                          edge_feat_dim=n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 16])\n"
     ]
    }
   ],
   "source": [
    "conv = NaiveGCN(dim=n_dim)\n",
    "gc_out = conv(g.x, g.edge_index)\n",
    "print(gc_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we have a more minute control over messaging passing scheme?\n",
    "### `message` method of `MessagePassing` class\n",
    "\n",
    "The NaiveGCN can be implemented without explicit message generation as the messages are the source node features. However, in many cases, we often employ more sophisticated message generation schemes that generates \"message\" from source, destination, and edge features (if applicable). For example, the following message generation scheme is used in \n",
    "[Edge Convolution](https://arxiv.org/abs/1801.07829):\n",
    "\n",
    "$$\n",
    "x'_i = \\max_{j \\in \\mathcal{N}(i)} h_\\theta\\left(x_i, x_j-x_i\\right),\n",
    "$$\n",
    "\n",
    "where $h_\\theta$ is an learnable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConv(MessagePassing):\n",
    "    \n",
    "    def __init__(self, dim:int):\n",
    "        super().__init__(aggr='max')\n",
    "        self.h = nn.Sequential(\n",
    "            nn.Linear(dim*2, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    \n",
    "    def message(self, x_i, x_j):\n",
    "        # By overriding this function, we can specify how messages are computed by\n",
    "        \n",
    "        # x_i is the source node and x_j is the target node\n",
    "        # x_i, x_j has shape [E, dim], where E is the number of edges\n",
    "        msg = torch.cat([x_i, x_j-x_i], dim=-1) # [E, 2*dim]\n",
    "        msg = self.h(msg) # [E, dim]\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 16])\n"
     ]
    }
   ],
   "source": [
    "conv = EdgeConv(dim=n_dim)\n",
    "gc_out = conv(g.x, g.edge_index)\n",
    "print(gc_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Update` method of `messagePassing` class\n",
    "\n",
    "The update method is used to update the node features using the aggregated messages. As a running example, we will consider a simplified Interaction Network layer, an iconic GNN model for learning the interaction between two objects (e.g., atoms, nodes, etc.) in a graph. The update function of the Interaction Network layer is defined as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "e'_{ij} &= f_\\theta(x_i, x_j, e_{ij}), \\\\\n",
    "x'_{i} &= g_\\theta(x_i, \\sum_{j \\in \\mathcal{N}(i)} e'_{ij}),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $f_\\theta$ and $g_\\theta$ are edge and node updater, respectively. The updaters are often implemented with learnable functions (e.g., MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionNetworkLayer(MessagePassing):\n",
    "    \n",
    "    def __init__(self, dim:int):\n",
    "        super().__init__(aggr='add')\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim), # Assuming edge are node features are of same dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        \n",
    "        self.g = nn.Sequential(\n",
    "            nn.Linear(dim*2, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        updated_ef = self.edge_update(x, edge_index, edge_attr)\n",
    "        updated_nf = self.propagate(edge_index, x=x, edge_attr=edge_attr)    \n",
    "        return updated_nf, updated_ef\n",
    "    \n",
    "    def edge_update(self, x, edge_index, edge_attr):\n",
    "        row, col = edge_index\n",
    "        x_i, x_j = x[row], x[col] # src and dst node features\n",
    "        return self.f(torch.cat([x_i, x_j, edge_attr], dim=-1)) # Eq (1)\n",
    "\n",
    "    # Eq (2) related\n",
    "    def message(self, edge_attr):\n",
    "        return edge_attr\n",
    "    \n",
    "    def update(self, aggr_msg, x): \n",
    "        # !!! The update function takes in the aggregated messages as the first argument !!!\n",
    "        # The other arguments are any arguments passed to the propagate function.\n",
    "        \n",
    "        # Maybe good to practice\n",
    "        # Try to pass additional arguments that is not passed to self.propagate such as 'y'\n",
    "        # by changing 'def update(self, aggs_msg, x)' to 'def update(self, aggs_msg, x, y)'\n",
    "        return self.g(torch.cat([x, aggr_msg], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 16]) torch.Size([12, 16])\n"
     ]
    }
   ],
   "source": [
    "conv = InteractionNetworkLayer(dim=n_dim)\n",
    "updated_nf, updated_ef = conv(g.x, g.edge_index, g.edge_attr)\n",
    "print(updated_nf.shape, updated_ef.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch200-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
