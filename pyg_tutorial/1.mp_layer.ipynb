{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from common.graph_gen import generate_random_graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MessagePassing` Base Class in PyG\n",
    "\n",
    "The `MessagePassing` base class in PyG implements the message passing scheme as follows:\n",
    "\n",
    "$$\n",
    "x'_i=\n",
    "\\underbrace{\n",
    "f_\\theta \\left(\\mathbf{x}_i,\n",
    "\\underbrace{ \n",
    "\\bigoplus_{j \\in \\mathcal{N}(i)} \n",
    "}_{\\text{(2) aggregation}}\n",
    "\\underbrace{\n",
    "g_\\theta\\left(\\mathbf{x}_i, \\mathbf{x}_j, \\mathbf{e}_{ij}\\right)\n",
    "}_{\\text{(1) message}}\n",
    "\\right)\n",
    "}_{\\text{(3) update}},\n",
    "$$\n",
    "\n",
    "where \n",
    "- $x_i \\in \\mathbb{R}^{d_n}$ and $x'_i \\in \\mathbb{R}^{d_n'}$ are the input and updated node features (embeddings), respectively.\n",
    "- $e_{ij} \\in \\mathbb{R}^{d_{e}}$ is the edge feature between nodes $i$ and $j$.\n",
    "- $\\bigoplus$ is a differentiable and permutation-invariant function (e.g., summation, mean, maximum).\n",
    "- $g_\\theta : \\mathbb{R}^{d_n} \\times \\mathbb{R}^{d_n} \\times \\mathbb{R}^{d_{e}} \\rightarrow \\mathbb{R}^{d''}$ is a trainable edge function (e.g., MLP).\n",
    "- $f_\\theta : \\mathbb{R}^{d_n} \\times \\mathbb{R}^{d''} \\rightarrow \\mathbb{R}^{d'_n}$ is a trainable mapping function (e.g., MLP).\n",
    "- $\\mathcal{N}(i)$ is the set of neighbors of node $i$.\n",
    "\n",
    "\n",
    "This message passing implementation is flexible and can be used to implement various GNN variants, including GCN, GAT, GraphSAGE, Interaction Layers, etc. In PyG, the `MessagePassing` triggers the message, aggregation, and update functions in the order of (1) -> (2) -> (3) while `propgate` the messages. In the following tutorial, we will demonstrate how to implement the components of message passing (message, aggregation, and update) to create different graph convolution layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very First Message Passing Layer; NaiveGCN\n",
    "\n",
    "We will use the `MessagePassing` base class to implement the NaiveGCN layer that performs the following message passing scheme:\n",
    "\n",
    "$$\n",
    "x'_i = \\sigma \\left(\\sum_{j \\in \\mathcal{N}(i)} \\left(W\\mathbf{x}_j+b\\right) \\right),\n",
    "$$\n",
    "where $\\sigma$ is an non-linear activations (e.g. ReLU, Tanh, SiLU, GELU, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes this is it! We are done with the implementation of NaiveGCN.\n",
    "\n",
    "class NaiveGCN(MessagePassing):\n",
    "    \n",
    "    def __init__(self, dim:int, act:'str'='ReLU'):\n",
    "        \n",
    "        super().__init__(aggr='add') # Aggregates messages with Summation (i.e., addition).        \n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "        self.act = getattr(nn, act)()\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.linear(x) # Perform Wx+b\n",
    "        x = self.propagate(x=x, edge_index=edge_index) # Propagate messages and Aggregate them with Summation\n",
    "        x = self.act(x) # Apply activation function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node = 11\n",
    "n_dim = 16\n",
    "\n",
    "g = generate_random_graph(num_node=num_node, \n",
    "                          node_feat_dim=n_dim,\n",
    "                          edge_feat_dim=n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 16])\n"
     ]
    }
   ],
   "source": [
    "conv = NaiveGCN(dim=n_dim)\n",
    "gc_out = conv(g.x, g.edge_index)\n",
    "print(gc_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we have a more minute control over messaging passing scheme?\n",
    "### `message` method of `MessagePassing` class\n",
    "\n",
    "The NaiveGCN can be implemented without explicit message generation as the messages are the source node features. However, in many cases, we often employ more sophisticated message generation schemes that generates \"message\" from source, destination, and edge features (if applicable). For example, the following message generation scheme is used in \n",
    "[Edge Convolution](https://arxiv.org/abs/1801.07829):\n",
    "\n",
    "$$\n",
    "x'_i = \\max_{j \\in \\mathcal{N}(i)} h_\\theta\\left(x_i, x_j-x_i\\right),\n",
    "$$\n",
    "\n",
    "where $h_\\theta$ is an learnable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConv(MessagePassing):\n",
    "    \n",
    "    def __init__(self, dim:int):\n",
    "        super().__init__(aggr='max')\n",
    "        self.h = nn.Sequential(\n",
    "            nn.Linear(dim*2, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    \n",
    "    def message(self, x_i, x_j):\n",
    "        # By overriding this function, we can specify how messages are computed by\n",
    "        \n",
    "        # x_i is the source node and x_j is the target node\n",
    "        # x_i, x_j has shape [E, dim], where E is the number of edges\n",
    "        msg = torch.cat([x_i, x_j-x_i], dim=-1) # [E, 2*dim]\n",
    "        msg = self.h(msg) # [E, dim]\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 16])\n"
     ]
    }
   ],
   "source": [
    "conv = EdgeConv(dim=n_dim)\n",
    "gc_out = conv(g.x, g.edge_index)\n",
    "print(gc_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Update` method of `messagePassing` class\n",
    "\n",
    "The update method is used to update the node features using the aggregated messages. As a running example, we will consider a simplified Interaction Network layer, an iconic GNN model for learning the interaction between two objects (e.g., atoms, nodes, etc.) in a graph. The update function of the Interaction Network layer is defined as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "e'_{ij} &= f_\\theta(x_i, x_j, e_{ij}), \\\\\n",
    "x'_{i} &= g_\\theta(x_i, \\sum_{j \\in \\mathcal{N}(i)} e'_{ij}),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $f_\\theta$ and $g_\\theta$ are edge and node updater, respectively. The updaters are often implemented with learnable functions (e.g., MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionNetworkLayer(MessagePassing):\n",
    "    \n",
    "    def __init__(self, dim:int):\n",
    "        super().__init__(aggr='add')\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim), # Assuming edge are node features are of same dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        \n",
    "        self.g = nn.Sequential(\n",
    "            nn.Linear(dim*2, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        updated_ef = self.edge_update(x, edge_index, edge_attr)\n",
    "        updated_nf = self.propagate(edge_index, x=x, edge_attr=edge_attr)    \n",
    "        return updated_nf, updated_ef\n",
    "    \n",
    "    def edge_update(self, x, edge_index, edge_attr):\n",
    "        row, col = edge_index\n",
    "        x_i, x_j = x[row], x[col] # src and dst node features\n",
    "        return self.f(torch.cat([x_i, x_j, edge_attr], dim=-1)) # Eq (1)\n",
    "\n",
    "    # Eq (2) related\n",
    "    def message(self, edge_attr):\n",
    "        return edge_attr\n",
    "    \n",
    "    def update(self, aggr_msg, x): \n",
    "        # !!! The update function takes in the aggregated messages as the first argument !!!\n",
    "        # The other arguments are any arguments passed to the propagate function.\n",
    "        \n",
    "        # Maybe good to practice\n",
    "        # Try to pass additional arguments that is not passed to self.propagate such as 'y'\n",
    "        # by changing 'def update(self, aggs_msg, x)' to 'def update(self, aggs_msg, x, y)'\n",
    "        return self.g(torch.cat([x, aggr_msg], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 11 but got size 8 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m conv \u001b[39m=\u001b[39m InteractionNetworkLayer(dim\u001b[39m=\u001b[39mn_dim)\n\u001b[0;32m----> 2\u001b[0m updated_nf, updated_ef  \u001b[39m=\u001b[39m conv(g\u001b[39m.\u001b[39;49mx, g\u001b[39m.\u001b[39;49medge_index, g\u001b[39m.\u001b[39;49medge_attr)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(updated_nf\u001b[39m.\u001b[39mshape, updated_ef\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200-py39/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[7], line 19\u001b[0m, in \u001b[0;36mInteractionNetworkLayer.forward\u001b[0;34m(self, x, edge_index, edge_attr)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index, edge_attr):\n\u001b[1;32m     18\u001b[0m     updated_ef \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_update(x, edge_index, edge_attr)\n\u001b[0;32m---> 19\u001b[0m     updated_nf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, edge_attr\u001b[39m=\u001b[39;49medge_attr)    \n\u001b[1;32m     20\u001b[0m     \u001b[39mreturn\u001b[39;00m updated_nf, updated_ef\n",
      "File \u001b[0;32m~/anaconda3/envs/torch200-py39/lib/python3.9/site-packages/torch_geometric/nn/conv/message_passing.py:492\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[0;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m         out \u001b[39m=\u001b[39m res\n\u001b[1;32m    491\u001b[0m update_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minspector\u001b[39m.\u001b[39mdistribute(\u001b[39m'\u001b[39m\u001b[39mupdate\u001b[39m\u001b[39m'\u001b[39m, coll_dict)\n\u001b[0;32m--> 492\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mupdate_kwargs)\n\u001b[1;32m    494\u001b[0m \u001b[39mif\u001b[39;00m decomposed_layers \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    495\u001b[0m     decomp_out\u001b[39m.\u001b[39mappend(out)\n",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m, in \u001b[0;36mInteractionNetworkLayer.update\u001b[0;34m(self, aggr_msg, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, aggr_msg, x): \n\u001b[1;32m     32\u001b[0m     \u001b[39m# !!! The update function takes in the aggregated messages as the first argument !!!\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[39m# The other arguments are any arguments passed to the propagate function.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[39m# Try to pass additional arguments that is not passed to self.propagate such as 'y'\u001b[39;00m\n\u001b[1;32m     37\u001b[0m     \u001b[39m# by changing 'def update(self, aggs_msg, x)' to 'def update(self, aggs_msg, x, y)'\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg(torch\u001b[39m.\u001b[39;49mcat([x, aggr_msg], dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 11 but got size 8 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "conv = InteractionNetworkLayer(dim=n_dim)\n",
    "updated_nf, updated_ef  = conv(g.x, g.edge_index, g.edge_attr)\n",
    "print(updated_nf.shape, updated_ef.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch200-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
