{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 1 : Implementing GNN layer using `MessagePasing` class\n",
    "\n",
    "In general, a Graph Neural Network (GNN) layer can be written as spectral-based or spatial-based methods. The spectral-based GNN layer is defined in the Fourier domain, while the spatial-based GNN layer is defined in the vertex domain. In often cases, majority mainstream GNN models are spatial-based methods due to the limitations of spectral-based methods, including the difficulty of generalization to unseen graphs and the high computational complexity. \n",
    "\n",
    "In this tutorial, we will focus on implementing spatial-based GNN layers (i.e., message passing networks) using the `MessagePassing` class of `PyG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from common.graph_gen import generate_random_graph"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `MessagePassing` Base Class in PyG\n",
    "\n",
    "The `MessagePassing` base class in PyG implements the message passing scheme as follows:\n",
    "\n",
    "$$\n",
    "x'_i=\n",
    "\\underbrace{\n",
    "f_\\theta \\left(\\mathbf{x}_i,\n",
    "\\underbrace{ \n",
    "\\bigoplus_{j \\in \\mathcal{N}(i)} \n",
    "}_{\\text{(2) aggregation}}\n",
    "\\underbrace{\n",
    "g_\\theta\\left(\\mathbf{x}_i, \\mathbf{x}_j, \\mathbf{e}_{ij}\\right)\n",
    "}_{\\text{(1) message}}\n",
    "\\right)\n",
    "}_{\\text{(3) update}},\n",
    "$$\n",
    "\n",
    "where \n",
    "- $x_i \\in \\mathbb{R}^{d_n}$ and $x'_i \\in \\mathbb{R}^{d_n'}$ are the input and updated node features (embeddings), respectively.\n",
    "- $d_n$ and $d'_n$ are the dimensions of node features before and after the update.\n",
    "- $e_{ij} \\in \\mathbb{R}^{d_{e}}$ is the edge feature between nodes $i$ and $j$.\n",
    "- $d_e$ is the dimension of edge features.\n",
    "- $\\bigoplus$ is a differentiable and permutation-invariant function (e.g., summation, mean, maximum).\n",
    "- $g_\\theta : \\mathbb{R}^{d_n} \\times \\mathbb{R}^{d_n} \\times \\mathbb{R}^{d_{e}} \\rightarrow \\mathbb{R}^{d''}$ is a trainable edge function (e.g., MLP).\n",
    "- $f_\\theta : \\mathbb{R}^{d_n} \\times \\mathbb{R}^{d''} \\rightarrow \\mathbb{R}^{d'_n}$ is a trainable mapping function (e.g., MLP).\n",
    "- $\\mathcal{N}(i)$ is the set of neighbors of node $i$.\n",
    "\n",
    "\n",
    "This message passing implementation is flexible and can be used to implement various GNN variants, including GCN, GAT, GraphSAGE, Interaction Layers, etc. \n",
    "\n",
    "In PyG, the `MessagePassing` triggers the (1) message, (2) message aggregation, and (3) node update functions in the order of (1) -> (2) -> (3) while `propgate` the messages. In the following tutorial, we will demonstrate how to implement the components of message passing (message, aggregation, and update) to create different graph convolution layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Very First Message Passing Layer; NaiveGCN\n",
    "\n",
    "We will use the `MessagePassing` base class to implement the NaiveGCN layer that performs the following message passing scheme:\n",
    "\n",
    "$$\n",
    "x'_i = \\sigma \\left(\\sum_{j \\in \\mathcal{N}(i)} \\left(W\\mathbf{x}_j+b\\right) \\right),\n",
    "$$\n",
    "where $\\sigma$ is an non-linear activations (e.g. ReLU, Tanh, SiLU, GELU, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes this is it! We are done with the implementation of NaiveGCN.\n",
    "\n",
    "class NaiveGCNConv(MessagePassing):\n",
    "    \n",
    "    def __init__(self, dim:int, act:'str'='ReLU'):\n",
    "        \n",
    "        super().__init__(aggr='add') # Aggregates messages with Summation (i.e., addition).        \n",
    "        self.linear = nn.Linear(dim, dim)\n",
    "        self.act = getattr(nn, act)()\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.linear(x) # Perform Wx+b\n",
    "        x = self.propagate(x=x, edge_index=edge_index) # Propagate messages and Aggregate them with Summation\n",
    "        x = self.act(x) # Apply activation function\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node = 5\n",
    "n_dim = 16\n",
    "\n",
    "g = generate_random_graph(num_node=num_node, \n",
    "                          p_edges=0.5,\n",
    "                          node_feat_dim=n_dim,\n",
    "                          edge_feat_dim=n_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 16])\n"
     ]
    }
   ],
   "source": [
    "conv = NaiveGCNConv(dim=n_dim)\n",
    "gc_out = conv(g.x, g.edge_index)\n",
    "print(gc_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can we have a more fine-grained control over messaging passing scheme?\n",
    "\n",
    "As explained earlier, we can generally manipulate (1) message generation routine,\n",
    "(2) message aggregation routine, and (3) node update routine to implement GNN layers.\n",
    "\n",
    "In the following, we will implement such (sub) routines in `MessagePassing` class.\n",
    "Generally, (1) is done by overridding `message` method, (2) is done by specifying `aggr` in `MessagePassing` class's constructor, and (3) is done by overridding `update` method.\n",
    "\n",
    "### `message` method of `MessagePassing` class\n",
    "\n",
    "The NaiveGCN can be implemented without explicit message generation as the messages are the source node features. However, in many cases, we often employ more sophisticated message generation schemes that generates \"message\" from source, destination, and edge features (if applicable). For example, the following message generation scheme is used in \n",
    "[Edge Convolution](https://arxiv.org/abs/1801.07829):\n",
    "\n",
    "$$\n",
    "x'_i = \\max_{j \\in \\mathcal{N}(i)} h_\\theta\\left(x_i, x_j-x_i\\right),\n",
    "$$\n",
    "\n",
    "where $h_\\theta$ is an learnable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConv(MessagePassing):\n",
    "    \n",
    "    def __init__(self, dim:int):\n",
    "        super().__init__(aggr='max')\n",
    "        self.h = nn.Sequential(\n",
    "            nn.Linear(dim*2, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        return self.propagate(edge_index, x=x)\n",
    "    \n",
    "    def message(self, x_i, x_j):\n",
    "        # By overriding this function, we can specify how messages are computed by\n",
    "        \n",
    "        # x_i is the source node and x_j is the target node\n",
    "        # x_i, x_j has shape [E, dim], where E is the number of edges\n",
    "        msg = torch.cat([x_i, x_j-x_i], dim=-1) # [E, 2*dim]\n",
    "        msg = self.h(msg) # [E, dim]\n",
    "        return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 16])\n"
     ]
    }
   ],
   "source": [
    "conv = EdgeConv(dim=n_dim)\n",
    "gc_out = conv(g.x, g.edge_index)\n",
    "print(gc_out.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Update` method of `MessagePassing` class\n",
    "\n",
    "The update method is used to update the node features using the aggregated messages. As a running example, we will consider a simplified Interaction Network layer, an iconic GNN model for learning the interaction between two objects (e.g., atoms, nodes, etc.) in a graph. The update function of the Interaction Network layer is defined as follows:\n",
    "$$\n",
    "\\begin{align}\n",
    "e'_{ij} &= f_\\theta(x_i, x_j, e_{ij}), \\\\\n",
    "x'_{i} &= g_\\theta(x_i, \\sum_{j \\in \\mathcal{N}(i)} e'_{ij}),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $f_\\theta$ and $g_\\theta$ are edge and node updater, respectively. The updaters are often implemented with learnable functions (e.g., MLP)\n",
    "\n",
    "Unlike `NaiveGCNConv` or `EdgeConv`, the Interaction Network Layer requires to update the edge features to perform node updates. To take account the edge update, we additionally implement `edge_update` method in the `MessagePassing` class. This `edge_update` method is called inside of `edge_updater` method that is already defined in `MessagePassing` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionNetworkLayer(MessagePassing):\n",
    "    \n",
    "    def __init__(self, dim:int):\n",
    "        super().__init__(aggr='add')\n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim), # Assuming edge are node features are of same dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        \n",
    "        self.g = nn.Sequential(\n",
    "            nn.Linear(dim*2, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        updated_ef = self.edge_updater(edge_index=edge_index, x=x, edge_attr=edge_attr)\n",
    "        updated_nf = self.propagate(edge_index=edge_index, \n",
    "                                    x=x, edge_attr=updated_ef)    \n",
    "        return updated_nf, updated_ef\n",
    "    \n",
    "    def edge_update(self, x, edge_index, edge_attr):\n",
    "        row, col = edge_index\n",
    "        x_i, x_j = x[row], x[col] # src and dst node features\n",
    "        return self.f(torch.cat([x_i, x_j, edge_attr], dim=-1)) # Eq (1)\n",
    "\n",
    "    # Eq (2) related\n",
    "    def message(self, edge_attr):\n",
    "        return edge_attr\n",
    "    \n",
    "    def update(self, aggr_msg, x): \n",
    "        # !!! The update function takes in the aggregated messages as the first argument !!!\n",
    "        # The other arguments are any arguments passed to the propagate function.\n",
    "        \n",
    "        # Maybe good to practice\n",
    "        # Try to pass additional arguments that is not passed to self.propagate such as 'y'\n",
    "        # by changing 'def update(self, aggs_msg, x)' to 'def update(self, aggs_msg, x, y)'\n",
    "        return self.g(torch.cat([x, aggr_msg], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 16]) torch.Size([12, 16])\n"
     ]
    }
   ],
   "source": [
    "conv = InteractionNetworkLayer(dim=n_dim)\n",
    "updated_nf, updated_ef = conv(g.x, g.edge_index, g.edge_attr)\n",
    "print(updated_nf.shape, updated_ef.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `aggr` argument of `messagePassing` class's constructor\n",
    "\n",
    "### Implementing `AttentiveInteractionLayer` with Advanced aggregation\n",
    "\n",
    "So far, we've considered to use \"simple\" aggregation methods in aggregating messages. For instance, attention-based aggregations\n",
    "Luckily, PyG provides a set of aggregation methods that can be used to implement various GNN variants. For example, lets implement attentive aggregation with `aggr.AttentionalAggregation` Formally, the following layer performs the following message passing scheme:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "e'_{ij} &= f_\\theta(x_i, x_j, e_{ij}), \\\\\n",
    "w_{ij} &= \\text{softmax}_j \\left(\\text{gate}_\\theta(e'_{ij}) \\right), \\\\\n",
    "x'_{i} &= g_\\theta(x_i, \\sum_{j \\in \\mathcal{N}(i)} w_{ij} e'_{ij}),\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import aggr\n",
    "\n",
    "class AttentiveINLayer(MessagePassing):\n",
    "    \n",
    "    def __init__(self, dim:int):\n",
    "        \n",
    "        gate_nn = nn.Sequential(\n",
    "            nn.Linear(dim, dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(dim, 1)\n",
    "        )\n",
    "            \n",
    "        super().__init__(aggr=aggr.AttentionalAggregation(gate_nn=gate_nn))\n",
    "        \n",
    "        self.f = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim), # Assuming edge are node features are of same dimension\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "        \n",
    "        self.g = nn.Sequential(\n",
    "            nn.Linear(dim*2, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        updated_ef = self.edge_updater(edge_index=edge_index, x=x, edge_attr=edge_attr)\n",
    "        updated_nf = self.propagate(edge_index, x=x, edge_attr=updated_ef)    \n",
    "        return updated_nf, updated_ef\n",
    "    \n",
    "    def edge_update(self, x, edge_index, edge_attr):\n",
    "        row, col = edge_index\n",
    "        x_i, x_j = x[row], x[col] # src and dst node features\n",
    "        return self.f(torch.cat([x_i, x_j, edge_attr], dim=-1)) # Eq (1)\n",
    "\n",
    "    def message(self, edge_attr):\n",
    "        return edge_attr\n",
    "    \n",
    "    def update(self, aggr_msg, x): \n",
    "        return self.g(torch.cat([x, aggr_msg], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 16]) torch.Size([12, 16])\n"
     ]
    }
   ],
   "source": [
    "conv = AttentiveINLayer(dim=n_dim)\n",
    "updated_nf, updated_ef = conv(g.x, g.edge_index, g.edge_attr)\n",
    "print(updated_nf.shape, updated_ef.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch200-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
