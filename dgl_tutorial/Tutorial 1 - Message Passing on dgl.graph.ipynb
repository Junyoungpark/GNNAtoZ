{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message passing framework of `dgl`\n",
    "\n",
    "One significant advantage of `dgl` over `Pytorch Geometric` is flexible message passing frameworks. The message passing frameworks allows the user to come up with user-defined-message routings. Such features are beneficial for designing a sophisticated routing mechanism in MARL applications.\n",
    "\n",
    "In this tutorial, we will not cover the advanced usage of the message passing framework of `dgl.` For more details, you can refer to this [link](https://docs.dgl.ai/guide/message.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = torch.tensor([0, 0, 0, 1]), torch.tensor([1, 2, 3, 3])\n",
    "g = dgl.graph((u, v), num_nodes=8)\n",
    "g = dgl.add_self_loop(g)\n",
    "\n",
    "node_feat_dim = 32 # the node feature dim\n",
    "edge_feat_dim = 3 # the edge feature dim\n",
    "\n",
    "g.ndata['feat'] = torch.randn(g.number_of_nodes(), node_feat_dim)\n",
    "g.edata['feat'] = torch.randn(g.number_of_edges(), edge_feat_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple GCN in message passing framework\n",
    "\n",
    "GCN is the most famous and generally works well GNN model. Here we define a simple-to-implement GCN and implement the GCN layer with `dgl`'s message passing framework. The simple GCN is a variant of GCN that uses the different $A$ from the original derivation.\n",
    "\n",
    "A simplified GCN layer can be defined as:\n",
    "\n",
    "$$H^{(l+1)} = \\sigma(AW^{(l)} H^{(l)}+ b^{(l)})$$\n",
    "\n",
    "Here the $l$ indicates the layer index of GCN and $H^{(l)}$ is the $l$-th layer input feature. By definition $H^{(0)}$ is the input feature $V$. $W^{(l)}$ and $b^{(l)}$ are the learning paramters of $l$-th GCN layer. $A$ is the adjacency matrix of the input graph.\n",
    "\n",
    "> Disclaimer: The original formulation GCN has no bias term $b^{(l)}$ because the existence of bias term makes unable to use the trained GCN models in the different sizes of graphs from the training cases.\n",
    "\n",
    "Checking the math above with the matrix multiplication would help you to understand what happen in the computation of GCN. Assume $n$ is the number of nodes in the input graph and $p^{(l)}$ and $q^{(l)}$ are the input and output feature dimension respectively. Then the adjacency $A \\in \\mathbb{R}^{n \\times n}$, weight matrix $W^{(l)} \\in \\mathbb{R}^{p^{(l)} \\times q^{(l)}}$, the input feature $H^{(l)} \\in \\mathbb{R}^{n \\times p^{(l)}}$ and the bias $b \\in \\mathbb{R}^{n \\times q^{(l)}}$. It becomes again clear that having the bias term disables the GCN to be used for differently sized graphs.\n",
    "\n",
    "## Message passing reformulation of the simple GCN\n",
    "\n",
    "Understanding the GCN operations from the perspective of given node $i$ provides you a good insight for implementing GCN within the message passing framework of `dgl.` In the single node perspective, the update rule of GCN can be re-written as follows:\n",
    "\n",
    "$$h^{(l+1)}_i = \\sigma(\\sum_{j \\in \\mathcal{N}(i)} z_j + b^{(l)}_i) $$\n",
    "\n",
    "where the $\\mathcal{N}(i)$ is the index set of node $i$'s neighborhood. The fused feature matrix $Z^{(l)}$ is defined as product of weight $W^{(l)}$ and input feature $H^{(l)}$. $z_j$ is the row vector of $Z$. By stacking all $h^{(l+1)}$, we can attain $H^{(l)}$, which is the exact outcome of the simplified GCN.\n",
    "\n",
    "In the following cell, let us code the message-passing-formulate version of the simple GCN within `dgl`'s message framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassingGCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim: int, \n",
    "                 output_dim: int):\n",
    "        super(MessagePassingGCN, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=input_dim,\n",
    "                                out_features=output_dim, bias=False)\n",
    "        \n",
    "    def forward(self, g, nf):        \n",
    "        g = g.local_var() # make a local graph\n",
    "        z = self.linear(nf) # compute WX -> Z\n",
    "        g.ndata['z'] = z\n",
    "        \n",
    "        g.pull(v=g.nodes(),\n",
    "               message_func=self.msg_func,\n",
    "               reduce_func=self.reduce_func)\n",
    "        \n",
    "        # The operations happens in `g.pull`\n",
    "        # 1. Message generation: In this case, the message is \"source node feature.\"\n",
    "        # 2. 'Push' the message to the destination nodes\n",
    "        # 3. 'Reduce' the messages from the destination nodes\n",
    "        # 4. Perform node update: In this case, we don't do.\n",
    "        \n",
    "        # For further details, refer to the dgl's API documents.        \n",
    "        return g.ndata['h']\n",
    "        \n",
    "    def msg_func(self, edges):        \n",
    "        return {'z': edges.src['z']}\n",
    "    \n",
    "    def reduce_func(self, nodes):\n",
    "        incoming_msg = nodes.mailbox['z'] # [#.nodes x # incomings x # feat. dim]\n",
    "        reduced_msg = incoming_msg.sum(dim=1) # perform AZ\n",
    "        return {'h' : reduced_msg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_out_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = MessagePassingGCN(node_feat_dim, gc_out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "h_updated = gc(g, g.ndata['feat'])\n",
    "print(h_updated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.82 ms ± 146 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "h_updated = gc(g, g.ndata['feat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A slightly optimized verision of the simple GCN with `dgl.function`\n",
    "\n",
    "`dgl` is not only a computational framework that supporting versatile message passing frameworks but also indeed optimized. Luckily, almost every basic arithmetic operations, such as weighted sums, top-k operations have implemented already in `dgl.function` package. The `dgl.function` also supports graph-readouts. For detailed explanations, please refer to this [link](https://docs.dgl.ai/guide/message.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassingGCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim: int, \n",
    "                 output_dim: int):\n",
    "        super(MessagePassingGCN, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=input_dim,\n",
    "                                out_features=output_dim, bias=False)\n",
    "        \n",
    "        self.msg_func = dgl.function.copy_src('z','z')\n",
    "        self.reduce_func = dgl.function.sum('z','h')\n",
    "        \n",
    "    def forward(self, g, nf):        \n",
    "        g = g.local_var() # make a local graph\n",
    "        z = self.linear(nf) # compute WX -> Z\n",
    "        g.ndata['z'] = z\n",
    "        \n",
    "        # Send source node features to the destination nodes\n",
    "        g.pull(v=g.nodes(),\n",
    "               message_func=self.msg_func,\n",
    "               reduce_func=self.reduce_func)\n",
    "        return g.ndata['h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = MessagePassingGCN(node_feat_dim, gc_out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "h_updated = gc(g, g.ndata['feat'])\n",
    "print(h_updated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.36 ms ± 8.49 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "h_updated = gc(g, g.ndata['feat'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
