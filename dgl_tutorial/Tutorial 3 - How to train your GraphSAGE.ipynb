{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train your GraphSAGE\n",
    "\n",
    "So far we learned 'How to build your own graph', and 'How to build a graph neural network', and 'How to perform forward propagation with the GNN\".\n",
    "In this tutorial, we will learn \"How to load/use famous graph benchmark dataset\", and \"How to train graph neural network model\".\n",
    "\n",
    "The content of this tutorial is orignally written by dgl team. The original documentation can be found from [here](https://docs.dgl.ai/guide/training-node.html#guide-training-node-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import torch.nn as nn\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST of GNNs : `CiteseerGraphDataset`\n",
    "\n",
    "`CiteseerGraphDataset` is a the famous graph benchmark datset. `CiteseerGraphDataset` contains a  scientific publications citetation graph.\n",
    "The node feature are predefined vectors with dimensions of 3703. The target task is to predict interger-valued labels of nodes. `CiteseerGraphDataset` contains\n",
    "six different node labels. \n",
    "\n",
    "Out of 3327 nodes in the orignal graph, total 1620 nodes are selected to be used during training, validating, and testing your model.\n",
    "1. 120 nodes can be used as the training nodes where you can directly compute the cross-entropy losses.\n",
    "2. 500 nodes are reserved for validation\n",
    "3. 1000 nodes are reserved for testing\n",
    "\n",
    "`dgl` provides `CiteseerGraphDataset` with everything is preprocesed as mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from cache failed, re-processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\joon0\\anaconda3\\envs\\torch160\\lib\\site-packages\\dgl\\data\\citation_graph.py:258: RuntimeWarning: divide by zero encountered in power\n",
      "  r_inv = np.power(rowsum, -1).flatten()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 3327\n",
      "  NumEdges: 9228\n",
      "  NumFeats: 3703\n",
      "  NumClasses: 6\n",
      "  NumTrainingSamples: 120\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done saving data into cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset = dgl.data.CiteseerGraphDataset()\n",
    "graph = dataset[0] # since it only has one graph :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Graph SAGE model\n",
    "\n",
    "Let's build simple Graph SAGE model with `dgl` and `pytorch`. The simple GraphSAGE model defined as follows:\n",
    "$$h=\\text{GraphSAGE}^{(2)}(\\text{ReLU}((\\text{GraphSAGE}^{(1)}(\\mathcal{G}, X))))$$\n",
    "\n",
    "With the `dgl.nn` you can code up the above model with few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, hid_feats, out_feats):\n",
    "        super().__init__()\n",
    "        self.conv1 = dglnn.SAGEConv(in_feats=in_feats, \n",
    "                                    out_feats=hid_feats, \n",
    "                                    aggregator_type='mean')\n",
    "        self.conv2 = dglnn.SAGEConv(in_feats=hid_feats, \n",
    "                                    out_feats=out_feats, \n",
    "                                    aggregator_type='mean')\n",
    "\n",
    "    def forward(self, graph, inputs):\n",
    "        # inputs are features of nodes\n",
    "        h = self.conv1(graph, inputs)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(graph, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature preparation\n",
    "\n",
    "To train the neural network models in supervised fashion, we need to have input feature and the corresponding target. \n",
    "Don't foreget that we must use the subset of node feature and corresponding the labels for training!\n",
    "Similary, we have to use only the subsets of feature and labels for validation and testing. Here, we will see\n",
    "how can we get the right subset of features and labels for the purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = graph.ndata['feat']\n",
    "node_labels = graph.ndata['label']\n",
    "train_mask = graph.ndata['train_mask']\n",
    "valid_mask = graph.ndata['val_mask']\n",
    "test_mask = graph.ndata['test_mask']\n",
    "n_features = node_features.shape[1]\n",
    "n_labels = int(node_labels.max().item() + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, graph, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(graph, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = torch.max(logits, dim=1)\n",
    "        correct = torch.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7948145866394043\n",
      "1.7812025547027588\n",
      "1.7674504518508911\n",
      "1.7531793117523193\n",
      "1.7383815050125122\n",
      "1.7231699228286743\n",
      "1.7076141834259033\n",
      "1.6916249990463257\n",
      "1.675097107887268\n",
      "1.6580389738082886\n"
     ]
    }
   ],
   "source": [
    "model = SAGE(in_feats=n_features, hid_feats=100, out_feats=n_labels)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    # forward propagation by using all nodes\n",
    "    logits = model(graph, node_features)\n",
    "    # compute loss\n",
    "    loss = F.cross_entropy(logits[train_mask], node_labels[train_mask])\n",
    "    # compute validation accuracy\n",
    "    acc = evaluate(model, graph, node_features, node_labels, valid_mask)\n",
    "    # backward propagation\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    print(loss.item())\n",
    "\n",
    "    # Save model if necessary.  Omitted in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
