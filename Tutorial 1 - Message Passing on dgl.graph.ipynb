{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message passing framework of `dgl`\n",
    "\n",
    "One major advantage of `dgl` over `Pytorch Geomteric` is flexible message passing frameworks. The message passing frameworks allows the user to come up with user-defined-message routings easily. Such features are extremely helpful for designing sophistacted routing mechanism in MARL applications.\n",
    "\n",
    "In this tutorial, we will not cover the advanced usage of the message passing framework of `dgl`. For more details, you can refer this [link](https://docs.dgl.ai/guide/message.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = torch.tensor([0, 0, 0, 1]), torch.tensor([1, 2, 3, 3])\n",
    "g = dgl.graph((u, v), num_nodes=8)\n",
    "g = dgl.add_self_loop(g)\n",
    "\n",
    "node_feat_dim = 32 # the node feature dim\n",
    "edge_feat_dim = 3 # the edge feature dim\n",
    "\n",
    "g.ndata['feat'] = torch.randn(g.number_of_nodes(), node_feat_dim)\n",
    "g.edata['feat'] = torch.randn(g.number_of_edges(), edge_feat_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple GCN in message passing framework\n",
    "\n",
    "GCN is the most famous and generally works well GNN model. Here we define a simple-to-implement GCN and implement the GCN layer with `dgl`'s message passing framework. The simple GCN is a variant of GCN that use the diffrent $A$ from the original derivation.\n",
    "\n",
    "A simplified GCN layer can be defined as:\n",
    "\n",
    "$$H^{(l+1)} = \\sigma(AW^{(l)} H^{(l)}+ b^{(l)})$$\n",
    "\n",
    "Here the $l$ indicates the layer index of GCN and $H^{(l)}$ is the $l$-th layer input feature. By definition $H^{(0)}$ is the input feature $V$. $W^{(l)}$ and $b^{(l)}$ are the learning paramters of $l$-th GCN layer. $A$ is the adjacency matrix of the input graph.\n",
    "\n",
    "\n",
    "> Disclaimer: The original formulation GCN has not bias term $b^{(l)}$ because the existence of bias term makes inable to use the trained GCN models in the different size of graphs from the training cases.\n",
    "\n",
    "Checking the math above with the matrix multiplication would help you to understand what happen in the computation of GCN. Assume $n$ is the number of nodes in the input graph and $p^{(l)}$ and $q^{(l)}$ are the input and output feature dimension respectively. Then the adjacency $A \\in \\mathbb{R}^{n \\times n}$, weight matrix $W^{(l)} \\in \\mathbb{R}^{p^{(l)} \\times q^{(l)}}$, the input feature $H^{(l)} \\in \\mathbb{R}^{n \\times p^{(l)}}$ and the bias $b \\in \\mathbb{R}^{n \\times q^{(l)}}$. It becomes again clear that having the bias term disables the GCN to be used for differently sized graphs.\n",
    "\n",
    "## Message passing reformulation of the simple GCN\n",
    "\n",
    "To implement the GNN operations with message passing framework, it is important to understand the operations in the perspective of given node $i$.\n",
    "\n",
    "In the single node perspective, the update rule can be re-written as follows:\n",
    "\n",
    "$$h^{(l+1)}_i = \\sigma(\\sum_{j \\in \\mathcal{N}(i)} z_j + b^{(l)}_i) $$\n",
    "\n",
    "where the $\\mathcal{N}(i)$ is the index set of node $i$'s neighborhood. The fused feature matrix $Z^{(l)}$ is defined as product of weight $W^{(l)}$ and input feature $H^{(l)}$. $z_j$ is the row vector of $Z$. By stacking all $h^{(l+1)}$, we can attain $H^{(l)}$ which is the exact outcome of the simplied GCN.\n",
    "\n",
    "In the following cell, let's code the message passing reformulated version of the simple GCN within `dgl`'s message framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassingGCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim: int, \n",
    "                 output_dim: int):\n",
    "        super(MessagePassingGCN, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=input_dim,\n",
    "                                out_features=output_dim, bias=False)\n",
    "        \n",
    "    def forward(self, g, nf):        \n",
    "        g = g.local_var() # make a local graph\n",
    "        z = self.linear(nf) # compute WX -> Z\n",
    "        g.ndata['z'] = z\n",
    "        \n",
    "        # Send source node features to the destination nodes\n",
    "        g.pull(v=g.nodes(),\n",
    "               message_func=self.msg_func,\n",
    "               reduce_func=self.reduce_func)\n",
    "        return g.ndata['h']\n",
    "        \n",
    "    def msg_func(self, edges):        \n",
    "        return {'z': edges.src['z']}\n",
    "    \n",
    "    def reduce_func(self, nodes):\n",
    "        incoming_msg = nodes.mailbox['z'] # [#.nodes x # incomings x # feat. dim]\n",
    "        reduced_msg = incoming_msg.sum(dim=1) # perform AZ\n",
    "        return {'h' : reduced_msg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_out_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = MessagePassingGCN(node_feat_dim, gc_out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "h_updated = gc(g, g.ndata['feat'])\n",
    "print(h_updated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7 ms ± 592 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "h_updated = gc(g, g.ndata['feat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A slightly optimized verision of simple GCN with `dgl.function`\n",
    "\n",
    "`dgl` is not only a computational framework that supporting versatile message passing frameworks but also indeed optimized. Luckily, almost every basic arithmetic opertaions, such as weighted sums, top-k operataions has implemented already in `dgl.function` pacakage. The `dgl.function` also supports graph-readouts. For the detailed explanations, please refer this [link](https://docs.dgl.ai/guide/message.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessagePassingGCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim: int, \n",
    "                 output_dim: int):\n",
    "        super(MessagePassingGCN, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=input_dim,\n",
    "                                out_features=output_dim, bias=False)\n",
    "        \n",
    "        self.msg_func = dgl.function.copy_src('z','z')\n",
    "        self.reduce_func = dgl.function.sum('z','h')\n",
    "        \n",
    "    def forward(self, g, nf):        \n",
    "        g = g.local_var() # make a local graph\n",
    "        z = self.linear(nf) # compute WX -> Z\n",
    "        g.ndata['z'] = z\n",
    "        \n",
    "        # Send source node features to the destination nodes\n",
    "        g.pull(v=g.nodes(),\n",
    "               message_func=self.msg_func,\n",
    "               reduce_func=self.reduce_func)\n",
    "        return g.ndata['h']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = MessagePassingGCN(node_feat_dim, gc_out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 256])\n"
     ]
    }
   ],
   "source": [
    "h_updated = gc(g, g.ndata['feat'])\n",
    "print(h_updated.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.59 ms ± 38.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "h_updated = gc(g, g.ndata['feat'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
