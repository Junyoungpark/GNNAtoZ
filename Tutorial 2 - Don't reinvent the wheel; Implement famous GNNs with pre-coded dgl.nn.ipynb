{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `dgl.nn` the offical dgl implementations of the famous GNNs\n",
    "\n",
    "`dgl.nn` is the dgl package you want to check when you start your GNN projects. In `dgl.nn`, you can find highly optimized GNN layers that are ready to be used for general purposes. Let's recover dgl implemented GCNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import dgl.nn.pytorch.conv as dglconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = torch.tensor([0, 0, 0, 1]), torch.tensor([1, 2, 3, 3])\n",
    "g = dgl.graph((u, v), num_nodes=8)\n",
    "g = dgl.add_self_loop(g)\n",
    "\n",
    "node_feat_dim = 32 # the node feature dim\n",
    "edge_feat_dim = 3 # the edge feature dim\n",
    "\n",
    "g.ndata['feat'] = torch.randn(g.number_of_nodes(), node_feat_dim)\n",
    "g.edata['feat'] = torch.randn(g.number_of_edges(), edge_feat_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_out_dim = 16\n",
    "\n",
    "gc = dglconv.GraphConv(in_feats=node_feat_dim, \n",
    "                       out_feats=gc_out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_updated = gc(g, g.ndata['feat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that all? Yes, that is all! Super simple. \n",
    "\n",
    "## So what happens under the hood of `dglconv.GraphConv`?\n",
    "\n",
    "`dglconv.GraphConv` implements practically very important details of GCN.\n",
    "\n",
    "1. Dynamically computing normalized Laplacian matrix.\n",
    "2. Adaptive computation of $AXW$\n",
    "> Basically, checking the input-output dims and perform the matrix product so that the number of arithmetic computations becomes small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the famous gnn layers are implemented?\n",
    "\n",
    "In the `dgl.nn`, there exist various implementations of the famous GNNs. out of all the implementations, you may be happy that `dgl.nn` has implemented the followings:\n",
    "1. Graph convolution (GCN) `GraphConv`\n",
    "2. Graph attention networks (GAT) `GATConv`\n",
    "3. Graph SAGE `SAGEConv`\n",
    "4. Graph isomorphism networks (GIN) `GINConv`\n",
    "\n",
    "In this tutorial, let's check the `GATConv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing edge attented node features with `GATConv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat = dglconv.GATConv(in_feats=node_feat_dim,\n",
    "                      out_feats=gc_out_dim,\n",
    "                      num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_updated_gat = gat(g, g.ndata['feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 16])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_updated_gat.shape # [#.nodes x #. attn head x # out dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched graph computations\n",
    "\n",
    "Mini batch training is common practice in training neural network models for efficient computations. That is also the same for training GNN models. However, batched computations of GNN made be less intuitive compared to the tensor version of those.\n",
    "\n",
    "In the batched forward propagations (and also backward) for the tensor inputs, you explicitly consider the first dimension of your inputs are designated for the batch. e.g., $X \\in \\mathbb{R}^{b \\times p}$ where $b$ is the size of mini-batch and $p$ is the input feature dimension.\n",
    "\n",
    "How can we batch the graph and how to compute the features on the graphs in a batched fashion? This idea is simple. Build a block matrix of adjacent matrices, and each block component is for the graphs. \n",
    "\n",
    "Manually implementing the batched graph is painful for many reasons. You have to deal with all the node and edge indices, which block component comes from which graph, etc.\n",
    "\n",
    "## Instead, why don't we `dgl.batch` ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_g = dgl.batch([g, g])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the graph statistics of `batched_g`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs in the batched graphs : 2 \n",
      "\n",
      "Total number of nodes : 16\n",
      "Total number of edges : 24 \n",
      "\n",
      "Per graph number of nodes : [8, 8]\n",
      "Per graph number of edges : [12, 12] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of graphs in the batched graphs : {} \\n\".format(batched_g.batch_size))\n",
    "\n",
    "print(\"Total number of nodes : {}\".format(batched_g.num_nodes()))\n",
    "print(\"Total number of edges : {} \\n\".format(batched_g.num_edges()))\n",
    "n_nodes = [i.item() for i in batched_g.batch_num_nodes()]\n",
    "n_edges = [i.item() for i in batched_g.batch_num_edges()]\n",
    "\n",
    "print(\"Per graph number of nodes : {}\".format(n_nodes))\n",
    "print(\"Per graph number of edges : {} \\n\".format(n_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noteworthy that in DGL implementations (>= 0.4v), either batched or single graphs are a different instantiation of the same graph class. Therefore, the methods we've used for the single graph `g` are also usable for the batched graph `batched_g`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dgl.heterograph.DGLHeteroGraph, dgl.heterograph.DGLHeteroGraph)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(g), type(batched_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## computing with batched graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 16])\n"
     ]
    }
   ],
   "source": [
    "h_updated_batched = gc(batched_g, batched_g.ndata['feat'])\n",
    "print(h_updated_batched.shape) # take a look at that the first dimension of output is now doubled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph readouts\n",
    "\n",
    "Graph readout is a function that reduces the set of graph node (and/or edge) attributes to the single vector. Technically, any function that collapse the set of graph attributes to the single tensor is a graph readout function.\n",
    "\n",
    "In GNN applications, the usage of graph readout function is discovered from various contexts. For instance, assuming you want to predict the labels or scores of graphs, in chemical applications, predicting whether the given graph is toxic or not; in RL applications, predicting the value of a given graph represented state.\n",
    "\n",
    "In practice, you also want to put additional constraints so that the graph readout functions work as you meant. The additional constraint is permutation invariance. `max`, `min`, `mean`, `std` are the operations that preserve permutation invariancy of the input set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Graph readout functions\n",
    "\n",
    "We can also implement graph readout naively with `pytorch` only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16])\n",
      "tensor([ 1.2323, -3.9087, -1.3567,  1.5853, -0.7145,  1.8170, -1.3865,  2.2112,\n",
      "         0.4228,  0.2590, -4.3310, -2.2310, -5.3134, -0.5614, -0.3056, -0.3361],\n",
      "       grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "readout = h_updated.sum(dim=0)\n",
    "print(readout.shape)\n",
    "print(readout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Would the naive implementation also work for batched graphs?\n",
    "\n",
    "The answer is NO. If the input graph is batched, then we have to consider the partition of nodes/edge features and where each partition comes from. Instead of using the naive implementation, let us use the `dgl.reaout_nodes` or `dgl.reaout_edges`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.ndata['updated_h'] = h_updated\n",
    "readout = dgl.readout_nodes(g, 'updated_h', op='sum')\n",
    "#print(readout.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n",
      "tensor([[ 1.2323, -3.9087, -1.3567,  1.5853, -0.7145,  1.8170, -1.3865,  2.2112,\n",
      "          0.4228,  0.2590, -4.3310, -2.2310, -5.3134, -0.5614, -0.3056, -0.3361]],\n",
      "       grad_fn=<GSpMMBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(readout.shape)\n",
    "print(readout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what is `grad_fn=<GSpMMBackward>)`?\n",
    "\n",
    "As I explained before, `dgl` is well optimized in terms of code performance, memory usage; code execution speed; backward speed. `GSpMMBackward` is a backward optimized process of `dgl`s. According to the `dgl` official API reference, `GSpMMBackward` is fully backpropagate-able. So if you have any chance to do with graph readout, use the native `dgl` implementations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
