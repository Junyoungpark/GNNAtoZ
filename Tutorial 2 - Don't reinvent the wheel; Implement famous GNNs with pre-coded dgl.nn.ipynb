{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `dgl.nn` the offical dgl implementations of the famous GNNs\n",
    "\n",
    "`dgl.nn` is the dgl package that you want to check when you start your GNN projects. In `dgl.nn`, you can find highly optimized GNN layers that are ready to be used for general purposes. Let's recover dgl implemented GCNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import dgl.nn.pytorch.conv as dglconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, v = torch.tensor([0, 0, 0, 1]), torch.tensor([1, 2, 3, 3])\n",
    "g = dgl.graph((u, v), num_nodes=8)\n",
    "g = dgl.add_self_loop(g)\n",
    "\n",
    "node_feat_dim = 32 # the node feature dim\n",
    "edge_feat_dim = 3 # the edge feature dim\n",
    "\n",
    "g.ndata['feat'] = torch.randn(g.number_of_nodes(), node_feat_dim)\n",
    "g.edata['feat'] = torch.randn(g.number_of_edges(), edge_feat_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_out_dim = 256\n",
    "\n",
    "gc = dglconv.GraphConv(in_feats=node_feat_dim, \n",
    "                       out_feats=gc_out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.33 ms ± 980 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "h_updated = gc(g, g.ndata['feat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that all? Yes, that is all! Super simple. \n",
    "\n",
    "## So what happens under the hood of `dglconv.GraphConv`?\n",
    "\n",
    "`dglconv.GraphConv` implements practically very important details of GCN.\n",
    "\n",
    "1. Dynamically computing normalized Laplacian matrix.\n",
    "2. Adaptive computation of $AXW$\n",
    "> Basically, checking the input-output dims and perform the matrix product so that the number of arithmetic computations becomes small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What the famous gnn layers are implemented?\n",
    "\n",
    "In the `dgl.nn`, there exist various implementations of the famous GNNs. out of all the implementations, you may be happy that `dgl.nn` has implemented the followings:\n",
    "1. Graph convolution (GCN) `GraphConv`\n",
    "2. Graph attention networks (GAT) `GATConv`\n",
    "3. Graph SAGE `SAGEConv`\n",
    "4. Graph isomorphism networks (GIN) `GINConv`\n",
    "\n",
    "In this tutorial, let's check the `GATConv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing edge attented node features with `GATConv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gat = dglconv.GATConv(in_feats=node_feat_dim,\n",
    "                      out_feats=gc_out_dim,\n",
    "                      num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_updated = gat(g, g.ndata['feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_updated.shape # [#.nodes x #. attn head x # out dim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched graph computations\n",
    "\n",
    "Mini batch training is common practice in training neural network models for efficient computations. That is also the same for training GNN models. However, batched computations of GNN made be less intuitive compared to the tensor version of those.\n",
    "\n",
    "In the batched forward propagations (and also backward) for the tensor inputs, you explicitly consider the first dimension of your inputs are designated for the batch. e.g., $X \\in \\mathbb{R}^{b \\times p}$ where $b$ is the size of mini-batch and $p$ is the input feature dimension.\n",
    "\n",
    "How can we batch the graph and how to compute the features on the graphs in a batched fashion? This idea is simple. Build a block matrix of adjacent matrices, and each block component is for the graphs. \n",
    "\n",
    "Manually implementing the batched graph is painful for many reasons. You have to deal with all the node and edge indices, which block component comes from which graph, etc.\n",
    "\n",
    "## Instead, why don't we `dgl.batch` ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_g = dgl.batch([g, g])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the graph statistics of `batched_g`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs in the batched graphs : 2 \n",
      "\n",
      "Total number of nodes : 16\n",
      "Total number of edges : 24 \n",
      "\n",
      "Per graph number of nodes : [8, 8]\n",
      "Per graph number of edges : [12, 12] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of graphs in the batched graphs : {} \\n\".format(batched_g.batch_size))\n",
    "\n",
    "print(\"Total number of nodes : {}\".format(batched_g.num_nodes()))\n",
    "print(\"Total number of edges : {} \\n\".format(batched_g.num_edges()))\n",
    "n_nodes = [i.item() for i in batched_g.batch_num_nodes()]\n",
    "n_edges = [i.item() for i in batched_g.batch_num_edges()]\n",
    "\n",
    "print(\"Per graph number of nodes : {}\".format(n_nodes))\n",
    "print(\"Per graph number of edges : {} \\n\".format(n_edges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is noteworthy that in DGL implementations (>= 0.4v), either batched or single graph are different instantiation of same graph class. Therefore, the methods we've used for the single graph `g` are also usable for the batched graph `batched_g`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dgl.heterograph.DGLHeteroGraph, dgl.heterograph.DGLHeteroGraph)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(g), type(batched_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## computing with batched graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256])\n"
     ]
    }
   ],
   "source": [
    "h_updated = gc(batched_g, batched_g.ndata['feat'])\n",
    "print(h_updated.shape) # take a look at that the first dimension of output is now doubled."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
